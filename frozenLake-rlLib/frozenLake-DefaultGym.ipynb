{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "434e2086-47d6-415f-84d8-030ab0d89ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa918f6-1bfb-490b-99c4-4846fef657e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedFrozenLake(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "#         self.reward_dist_weight = reward_dist_weight\n",
    "#         self.reward_ctrl_weight = reward_ctrl_weight\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, r, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            reward = 10 if r > 0 else -5\n",
    "        else:\n",
    "            reward = -1\n",
    "        return obs, reward, done, info\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config):    \n",
    "    return WrappedFrozenLake(gym.make('FrozenLake-v1', is_slippery=False))  # return an env instance\n",
    "\n",
    "register_env(\"myenv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "7 -5 True {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = env_creator(env_config = {})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    print(obs, reward, done, info)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d875ff98-1145-45ce-b343-cd36683bc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(env=\"myenv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d8fae5e-4b53-4692-aa6a-7c7c2e23f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": \"myenv\",\n",
    "      #  \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "#             \"model\": {\n",
    "#             \"fcnet_hiddens\": [256, 800, 256],\n",
    "#             \"fcnet_activation\": \"relu\",\n",
    "        # },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 1000,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21addd5d-a56e-4755-aecf-4bc2301558f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 10:18:26,195\tWARNING deprecation.py:46 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `RepayBuffer.add()` instead. This will raise an error in the future!\n",
      "2022-07-13 10:18:26,226\tWARNING multi_agent_prioritized_replay_buffer.py:186 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "2022-07-13 10:18:26,238\tWARNING deprecation.py:46 -- DeprecationWarning: `replay` has been deprecated. Use `sample` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-10.063400576368876\n",
      "Iter: 1; avg. reward=-11.682978723404256\n",
      "Iter: 2; avg. reward=-4.406340057636887\n",
      "Iter: 3; avg. reward=3.713554987212276\n",
      "Iter: 4; avg. reward=-1.4\n",
      "Iter: 5; avg. reward=-2.0792079207920793\n",
      "Iter: 6; avg. reward=4.637809187279152\n",
      "Iter: 7; avg. reward=4.541436464088398\n",
      "Iter: 8; avg. reward=-0.55119825708061\n",
      "Iter: 9; avg. reward=4.420863309352518\n",
      "Iter: 10; avg. reward=2.35\n",
      "Iter: 11; avg. reward=-2.71\n",
      "Iter: 12; avg. reward=-8.79\n",
      "Iter: 13; avg. reward=-32.68996960486322\n",
      "Iter: 14; avg. reward=4.627722772277227\n",
      "Iter: 15; avg. reward=4.63745704467354\n",
      "Iter: 16; avg. reward=4.664220183486239\n",
      "Iter: 17; avg. reward=4.598181818181819\n",
      "Iter: 18; avg. reward=4.737945492662474\n",
      "Iter: 19; avg. reward=4.7328519855595665\n",
      "Iter: 20; avg. reward=4.423868312757202\n",
      "Iter: 21; avg. reward=-2.33\n",
      "Iter: 22; avg. reward=-0.06722689075630252\n",
      "Iter: 23; avg. reward=4.598591549295775\n",
      "Iter: 24; avg. reward=4.5703517587939695\n",
      "Iter: 25; avg. reward=4.649186256781194\n",
      "Iter: 26; avg. reward=4.67037037037037\n",
      "Iter: 27; avg. reward=4.600713012477718\n",
      "Iter: 28; avg. reward=4.654478976234004\n",
      "Iter: 29; avg. reward=4.446180555555555\n",
      "Iter: 30; avg. reward=4.670909090909091\n",
      "Iter: 31; avg. reward=4.691139240506329\n",
      "Iter: 32; avg. reward=4.732495511669659\n",
      "Iter: 33; avg. reward=4.635379061371841\n",
      "Iter: 34; avg. reward=4.591726618705036\n",
      "Iter: 35; avg. reward=4.5452930728241565\n",
      "Iter: 36; avg. reward=4.579710144927536\n",
      "Iter: 37; avg. reward=4.6768402154398565\n",
      "Iter: 38; avg. reward=4.752688172043011\n",
      "Iter: 39; avg. reward=4.621863799283154\n",
      "Iter: 40; avg. reward=4.8130671506352085\n",
      "Iter: 41; avg. reward=4.595588235294118\n",
      "Iter: 42; avg. reward=4.732472324723247\n",
      "Iter: 43; avg. reward=4.5985790408525755\n",
      "Iter: 44; avg. reward=4.667262969588551\n",
      "Iter: 45; avg. reward=4.688524590163935\n",
      "Iter: 46; avg. reward=4.71326164874552\n",
      "Iter: 47; avg. reward=4.661844484629294\n",
      "Iter: 48; avg. reward=4.69449378330373\n",
      "Iter: 49; avg. reward=4.677655677655678\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659419a5-2d52-49cd-a63e-2591d7e299da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/checkpoint_000050/checkpoint-50'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367f5bc-8895-4312-bebd-71c0de6387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load_checkpoint(\"model/checkpoint_000300/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91461ee2-8111-491e-97d1-9c7469a6e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "    env = env_creator({}) #gym.make(\"myenv-v1\")#\"FrozenLake-v1\")\n",
    "    #, #FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "#        display.clear_output(wait=True)\n",
    "        # print(\"num won: \", won, \" played: \", i, \"total reward: \", total_reward)\n",
    "        # env.render()\n",
    "\n",
    "        action = trainer.compute_single_action(obs,explore=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #print(obs, reward, done)\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "        if done: \n",
    "            rewardList.append(total_reward)\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "      \n",
    "    \n",
    "#print(rewardList, np.mean(rewardList))\n",
    "print( np.mean(rewardList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec0a2fed-4274-44a9-b75a-a382ca577ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2db7732f-6a45-43c2-b799-c23e2e6b45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Why is testing average mean reward is very low compared to training mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6c77f85-b1ac-40e7-861f-ce9bd63ee780",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a19b681-2ca3-4648-a9c8-26f59f45404f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_single_action(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81b51076-950e-4d67-99b6-e8ce1751a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = env_creator({})# gym.make(\"FrozenLake-v1\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    display.clear_output(wait=True)\n",
    "    obs, reward, done, _ = env.step(trainer.compute_single_action(obs,explore=True))\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    if done: \n",
    "        rewardList.append(total_reward)\n",
    "if reward > 0 :\n",
    "    won +=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56383a75-ac4d-492e-ae2e-b3838bc1f580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a8e6b-8858-439f-b81a-1ca7a86b7544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
