{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae92c19-99c3-4047-942a-015086300eb6",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] Code ran with flattened observation. How to run with nd-array observation? \n",
    "- [x] How to provide pytorch net that I trained earlier for fronzen lake.\n",
    "- [ ] Use ray tuner for hyperparamter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434e2086-47d6-415f-84d8-030ab0d89ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c30796f-3fe8-4d3e-9021-72d5c3662f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redifne this map to put starting and ending point in random location\n",
    "# TODO: Can we do this with decorators?\n",
    "def newGenerate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    Args:\n",
    "        size: size of each side of the grid\n",
    "        p: probability that a tile is frozen\n",
    "    Returns:\n",
    "        A random valid map\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res,sx,sy):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((sx, sy))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        sx = np.random.randint(size); sy = np.random.randint(size)\n",
    "        res[sx][sy] = \"S\"\n",
    "        gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        while res[gx][gy] == \"S\": # we don't want to overwrite the S\n",
    "            gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        res[gx][gy] = \"G\"   \n",
    "        valid = is_valid(res,sx,sy)\n",
    "    return [\"\".join(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5573b1-d96d-446c-a52a-119703605d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeWrapped(gym.Env):\n",
    "    def __init__(self,config):\n",
    "        self.size = 6 # grid size\n",
    "        nH = 10.    # desired number of hole: this is probabilistic\n",
    "        pH = 1 - nH/(self.size**2) # probability of  grid being a hole\n",
    "        desc = newGenerate_random_map(size= self.size, p= pH)\n",
    "        self.env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n",
    "        self.observation_space = gym.spaces.Box(0,1,shape=(3*self.size*self.size,),dtype=\"int32\")\n",
    "        self.action_space = self.env.action_space\n",
    "    def oneHot(self,s):\n",
    "        x = np.zeros(self.size*self.size).astype(\"int32\")\n",
    "        x[s] = 1\n",
    "        state_ = np.array([ x.reshape(self.size,self.size),\n",
    "                         np.array(self.env.desc == b\"F\").astype(\"int32\"),\n",
    "                         np.array(self.env.desc == b\"G\").astype(\"int32\")\n",
    "                          ])\n",
    "        return state_.reshape(3*self.size*self.size,)# + np.random.rand(state_.size).reshape(state_.shape)/10.\n",
    "    \n",
    "    def reset(self):\n",
    "        o = self.env.reset()\n",
    "        return self.oneHot(o)\n",
    "\n",
    "    def step(self, action):\n",
    "        o, r, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            if r > 0:\n",
    "                newReward = 10\n",
    "            else:\n",
    "                newReward = -2\n",
    "        else:\n",
    "            newReward = -1\n",
    "            \n",
    "        return self.oneHot(o), newReward , done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "FFFHFG\n",
      "FHFHFF\n",
      "FFFFFH\n",
      "FFF\u001b[41mH\u001b[0mFF\n",
      "FHFSHH\n",
      "FHFFFF\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeWrapped({})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090757ae-b518-4ab3-b94b-6d2a3a67d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = torch.device(\"cpu\")#\"cuda\"\n",
    "        #                            if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.mainLayer = nn.Sequential(\n",
    "                nn.Conv2d(3,12,kernel_size=3,stride= 1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(12,24,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(24,36,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Flatten(start_dim=-3,end_dim=-1),\n",
    "                nn.Linear(1296,1200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1200,64),\n",
    "                nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(64, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(64, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.mainLayer(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(-1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85977dc6-d028-45b9-a264-ef60d69a523e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1001], grad_fn=<AddBackward0>), [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 6\n",
    "test_model_torch = MyTorchModel(\n",
    "   obs_space=gym.spaces.Box(0,1,shape=(3,size,size,), dtype=np.int32),\n",
    "   action_space=gym.spaces.Discrete(4),\n",
    "   num_outputs=1,\n",
    "   model_config={},\n",
    "   name=\"MyModel\",\n",
    ")\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n",
    "\n",
    "obs = gym.spaces.Box(0,1,shape=(3,size,size)).sample()\n",
    "test_model_torch({\"obs\": torch.from_numpy(obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fae5e-4b53-4692-aa6a-7c7c2e23f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "            # \"model\": {\n",
    "            # \"fcnet_hiddens\": [256, 800, 256],\n",
    "            # \"fcnet_activation\": \"relu\",\n",
    "        \"model\": {\n",
    "        \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "        },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a38af8-e1d2-4d8f-8d8f-ffa22ed6d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": \"FrozenLake-v1\",#FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "            \"model\": {\n",
    "            \"fcnet_hiddens\": [256, 800, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        # \"model\": {\n",
    "        # \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        # \"custom_model_config\": {\n",
    "        #     #\"layers\": [128, 128],\n",
    "        # },\n",
    "        },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21addd5d-a56e-4755-aecf-4bc2301558f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):    \n\u001b[0;32m----> 2\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; avg. reward=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(50):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659419a5-2d52-49cd-a63e-2591d7e299da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367f5bc-8895-4312-bebd-71c0de6387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load_checkpoint(\"model/checkpoint_000300/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91461ee2-8111-491e-97d1-9c7469a6e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    #, #FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "#        display.clear_output(wait=True)\n",
    "        # print(\"num won: \", won, \" played: \", i, \"total reward: \", total_reward)\n",
    "        # env.render()\n",
    "\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #print(obs, reward, done)\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "        if done: \n",
    "            rewardList.append(total_reward)\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "      \n",
    "    \n",
    "#print(rewardList, np.mean(rewardList))\n",
    "print( np.mean(rewardList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7732f-6a45-43c2-b799-c23e2e6b45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Why is testing average mean reward is very low compared to training mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b51076-950e-4d67-99b6-e8ce1751a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5724d42d-71a2-47d9-9548-e777ce0af1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 15:25:23,986\tWARNING ppo.py:386 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068108)\u001b[0m 2022-07-11 15:25:26,630\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068108)\u001b[0m /home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068108)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068107)\u001b[0m 2022-07-11 15:25:26,586\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068107)\u001b[0m /home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068107)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068106)\u001b[0m 2022-07-11 15:25:26,637\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068106)\u001b[0m /home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068106)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2022-07-11 15:25:32,401\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-29.64285714285715\n",
      "Iter: 1; avg. reward=-6.936752136752133\n",
      "Iter: 2; avg. reward=-3.188829787234041\n",
      "Iter: 3; avg. reward=-2.164227642276424\n",
      "Iter: 4; avg. reward=-1.6125423728813568\n",
      "Played 1 episode; total-reward=-0.7000000000000004\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "\n",
    "# Define your problem using python and openAI's gym API:\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n",
    "\n",
    "    ---------------------\n",
    "    | S | 1 | 2 | 3 | G |   S=start; G=goal; corridor_length=5\n",
    "    ---------------------\n",
    "\n",
    "    Possible actions to chose from are: 0=left; 1=right\n",
    "    Observations are floats indicating the current field index, e.g. 0.0 for\n",
    "    starting position, 1.0 for the field next to the starting position, etc..\n",
    "    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = gym.spaces.Discrete(2)  # left and right\n",
    "        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\"\"\"\n",
    "        self.cur_pos = 0\n",
    "        # Return initial observation.\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given `action`\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, done-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "        # Walk left.\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Walk right.\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        # Set `done` flag when end of corridor (goal) reached.\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # +1 when goal reached, otherwise -1.\n",
    "        reward = 1.0 if done else -0.1\n",
    "        return [self.cur_pos], reward, done, {}\n",
    "\n",
    "\n",
    "# Create an RLlib Trainer instance.\n",
    "trainer = PPOTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": SimpleCorridor,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "            # Use corridor with 20 fields (including S and G).\n",
    "            \"corridor_length\": 20\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to move at least 19 times in the env to reach the goal and\n",
    "# each move gives us -0.1 reward (except the last move at the end: +1.0),\n",
    "# we can expect to reach an optimal episode reward of -0.1*18 + 1.0 = -0.8\n",
    "for i in range(5):\n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly different env here (len 10 instead of 20),\n",
    "# however, this should still work as the agent has (hopefully) learned\n",
    "# to \"just always walk right!\"\n",
    "env = SimpleCorridor({\"corridor_length\": 10})\n",
    "# Get the initial observation (should be: [0.0] for the starting position).\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = trainer.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "# Report results.\n",
    "print(f\"Played 1 episode; total-reward={total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4cb3386-425a-4485-ac7e-fe1f28bc718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3710000000000009\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    env = SimpleCorridor({\"corridor_length\": 20})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # Compute a single action, given the current observation\n",
    "        # from the environment.\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        # Apply the computed action in the environment.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Sum up rewards for reporting purposes.\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            rewardList.append(total_reward)\n",
    "        \n",
    "    # Report results.\n",
    "#    print(f\"Played 1 episode; total-reward={total_reward}\")\n",
    "print(np.mean(rewardList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5320b1-1c53-4842-a842-dbb7f42ceacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263d554-17d4-4c1d-8c6f-1863106c8330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
