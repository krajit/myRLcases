{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae92c19-99c3-4047-942a-015086300eb6",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] Code ran with flattened observation. How to run with nd-array observation? \n",
    "- [x] How to provide pytorch net that I trained earlier for fronzen lake.\n",
    "- [ ] Use ray tuner for hyperparamter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434e2086-47d6-415f-84d8-030ab0d89ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c30796f-3fe8-4d3e-9021-72d5c3662f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redifne this map to put starting and ending point in random location\n",
    "# TODO: Can we do this with decorators?\n",
    "def newGenerate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    Args:\n",
    "        size: size of each side of the grid\n",
    "        p: probability that a tile is frozen\n",
    "    Returns:\n",
    "        A random valid map\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res,sx,sy):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((sx, sy))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        sx = np.random.randint(size); sy = np.random.randint(size)\n",
    "        res[sx][sy] = \"S\"\n",
    "        gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        while res[gx][gy] == \"S\": # we don't want to overwrite the S\n",
    "            gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        res[gx][gy] = \"G\"   \n",
    "        valid = is_valid(res,sx,sy)\n",
    "    return [\"\".join(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5573b1-d96d-446c-a52a-119703605d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeWrapped(gym.Env):\n",
    "    def __init__(self,config):\n",
    "        self.size = 6 # grid size\n",
    "        nH = 10.    # desired number of hole: this is probabilistic\n",
    "        pH = 1 - nH/(self.size**2) # probability of  grid being a hole\n",
    "        desc = newGenerate_random_map(size= self.size, p= pH)\n",
    "        self.env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n",
    "        self.observation_space = gym.spaces.Box(0,1,shape=(3,self.size,self.size),dtype=\"int32\")\n",
    "        self.action_space = self.env.action_space\n",
    "    def oneHot(self,s):\n",
    "        x = np.zeros(self.size*self.size).astype(\"int32\")\n",
    "        x[s] = 1\n",
    "        state_ = np.array([ x.reshape(self.size,self.size).astype(\"int32\"),\n",
    "                         np.array(self.env.desc == b\"F\").astype(\"int32\"),\n",
    "                         np.array(self.env.desc == b\"G\").astype(\"int32\")\n",
    "                          ])\n",
    "        return state_.reshape(3,self.size,self.size,)# + np.random.rand(state_.size).reshape(state_.shape)/10.\n",
    "    \n",
    "    def reset(self):\n",
    "        o = self.env.reset()\n",
    "        return self.oneHot(o)\n",
    "\n",
    "    def step(self, action):\n",
    "        o, r, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            if r > 0:\n",
    "                newReward = 10\n",
    "            else:\n",
    "                newReward = -2\n",
    "        else:\n",
    "            newReward = -1\n",
    "            \n",
    "        return self.oneHot(o), newReward , done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "FHFFHF\n",
      "FFFFFF\n",
      "FFFFFF\n",
      "FFFFFG\n",
      "\u001b[41mH\u001b[0mSFFFF\n",
      "FHFFHH\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeWrapped({})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090757ae-b518-4ab3-b94b-6d2a3a67d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = torch.device(\"cpu\")#\"cuda\"\n",
    "        #                            if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.mainLayer = nn.Sequential(\n",
    "                nn.Conv2d(3,12,kernel_size=3,stride= 1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(12,24,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(24,36,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Flatten(start_dim=-3,end_dim=-1),\n",
    "                nn.Linear(1296,1200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1200,64),\n",
    "                nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(64, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(64, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.mainLayer(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(-1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85977dc6-d028-45b9-a264-ef60d69a523e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0702,  0.0210,  0.0069, -0.0752], grad_fn=<AddBackward0>), [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 6\n",
    "test_model_torch = MyTorchModel(\n",
    "   obs_space=gym.spaces.Box(0,1,shape=(3,size,size,), dtype=np.int32),\n",
    "   action_space=gym.spaces.Discrete(4),\n",
    "   num_outputs=4,\n",
    "   model_config={},\n",
    "   name=\"MyModel\",\n",
    ")\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n",
    "\n",
    "obs = gym.spaces.Box(0,1,shape=(3,size,size)).sample()\n",
    "test_model_torch({\"obs\": torch.from_numpy(obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8fae5e-4b53-4692-aa6a-7c7c2e23f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m 2022-07-15 12:29:27,040\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m 2022-07-15 12:29:27,085\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067793, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7b28b31930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067793)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m 2022-07-15 12:29:27,083\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m 2022-07-15 12:29:27,147\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m 2022-07-15 12:29:27,209\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067787, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f67a53f1960>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067787)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m 2022-07-15 12:29:27,151\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067798, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbfad379930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067798)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m 2022-07-15 12:29:27,186\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m 2022-07-15 12:29:27,230\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067788, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbf765b1900>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067788)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m 2022-07-15 12:29:27,220\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m 2022-07-15 12:29:27,239\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m 2022-07-15 12:29:27,244\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m 2022-07-15 12:29:27,212\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m 2022-07-15 12:29:27,221\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m 2022-07-15 12:29:27,268\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067792, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8145e7d930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067792)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m 2022-07-15 12:29:27,300\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067790, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7efe86509930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067790)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m 2022-07-15 12:29:27,288\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067795, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f99c53f9930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067795)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m 2022-07-15 12:29:27,256\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067794, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f94c9b45960>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067794)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m 2022-07-15 12:29:27,268\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067799, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f71034458d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067799)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m 2022-07-15 12:29:27,320\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m 2022-07-15 12:29:27,319\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m 2022-07-15 12:29:27,341\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m 2022-07-15 12:29:27,381\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067786, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0852771930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067786)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m 2022-07-15 12:29:27,369\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067797, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa5fa8d9930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067797)\u001b[0m RuntimeError: expected scalar type Int but found Float\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m 2022-07-15 12:29:27,383\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067796, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb335a7d960>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     return self._compute_action_helper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     q_vals = compute_q_values(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     input = module(input)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m     return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3067796)\u001b[0m RuntimeError: expected scalar type Int but found Float\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067786, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0852771930>)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n    self[policy_id] = class_(observation_space, action_space, merged_config)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n    self._initialize_loss_from_dummy_batch(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n    return self._compute_action_helper(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n    return func(self, *a, **k)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n    q_vals = compute_q_values(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n    model_out, state = model(input_dict, state_batches or [], seq_lens)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n    res = self.forward(restored, state or [], seq_lens)\n  File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n    input = module(input)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: expected scalar type Int but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDQNTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mFrozenLakeWrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframework\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"model\": {\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"fcnet_hiddens\": [256, 800, 256],\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"fcnet_activation\": \"relu\",\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMyTorchModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# for torch users: \"custom_model\": MyTorchModel\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom_model_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#\"layers\": [128, 128],\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_envs_per_worker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_function_tpl\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers_for_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:142\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     local_worker\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m ):\n\u001b[0;32m--> 142\u001b[0m     remote_spaces \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    148\u001b[0m         e[\u001b[38;5;241m0\u001b[39m]: (\u001b[38;5;28mgetattr\u001b[39m(e[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, e[\u001b[38;5;241m1\u001b[39m]), e[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m remote_spaces\n\u001b[1;32m    150\u001b[0m     }\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/worker.py:1833\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1833\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   1836\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3067786, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0852771930>)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n    self[policy_id] = class_(observation_space, action_space, merged_config)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 326, in __init__\n    self._initialize_loss_from_dummy_batch(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 904, in _initialize_loss_from_dummy_batch\n    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 335, in compute_actions_from_input_dict\n    return self._compute_action_helper(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n    return func(self, *a, **k)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 964, in _compute_action_helper\n    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 232, in get_distribution_inputs_and_class\n    q_vals = compute_q_values(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 411, in compute_q_values\n    model_out, state = model(input_dict, state_batches or [], seq_lens)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n    res = self.forward(restored, state or [], seq_lens)\n  File \"/tmp/ipykernel_3066638/2501841030.py\", line 41, in forward\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 141, in forward\n    input = module(input)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 447, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 443, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: expected scalar type Int but found Float"
     ]
    }
   ],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "            # \"model\": {\n",
    "            # \"fcnet_hiddens\": [256, 800, 256],\n",
    "            # \"fcnet_activation\": \"relu\",\n",
    "        \"model\": {\n",
    "        \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "        },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a38af8-e1d2-4d8f-8d8f-ffa22ed6d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": \"FrozenLake-v1\",#FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "            \"model\": {\n",
    "            \"fcnet_hiddens\": [256, 800, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        # \"model\": {\n",
    "        # \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        # \"custom_model_config\": {\n",
    "        #     #\"layers\": [128, 128],\n",
    "        # },\n",
    "        },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21addd5d-a56e-4755-aecf-4bc2301558f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):    \n\u001b[0;32m----> 2\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; avg. reward=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(50):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659419a5-2d52-49cd-a63e-2591d7e299da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367f5bc-8895-4312-bebd-71c0de6387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load_checkpoint(\"model/checkpoint_000300/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91461ee2-8111-491e-97d1-9c7469a6e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    #, #FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "#        display.clear_output(wait=True)\n",
    "        # print(\"num won: \", won, \" played: \", i, \"total reward: \", total_reward)\n",
    "        # env.render()\n",
    "\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #print(obs, reward, done)\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "        if done: \n",
    "            rewardList.append(total_reward)\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "      \n",
    "    \n",
    "#print(rewardList, np.mean(rewardList))\n",
    "print( np.mean(rewardList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7732f-6a45-43c2-b799-c23e2e6b45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Why is testing average mean reward is very low compared to training mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b51076-950e-4d67-99b6-e8ce1751a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5724d42d-71a2-47d9-9548-e777ce0af1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 15:25:23,986\tWARNING ppo.py:386 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068108)\u001b[0m 2022-07-11 15:25:26,630\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068108)\u001b[0m /home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068108)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068107)\u001b[0m 2022-07-11 15:25:26,586\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068107)\u001b[0m /home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068107)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068106)\u001b[0m 2022-07-11 15:25:26,637\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068106)\u001b[0m /home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1068106)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "2022-07-11 15:25:32,401\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-29.64285714285715\n",
      "Iter: 1; avg. reward=-6.936752136752133\n",
      "Iter: 2; avg. reward=-3.188829787234041\n",
      "Iter: 3; avg. reward=-2.164227642276424\n",
      "Iter: 4; avg. reward=-1.6125423728813568\n",
      "Played 1 episode; total-reward=-0.7000000000000004\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "\n",
    "# Define your problem using python and openAI's gym API:\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n",
    "\n",
    "    ---------------------\n",
    "    | S | 1 | 2 | 3 | G |   S=start; G=goal; corridor_length=5\n",
    "    ---------------------\n",
    "\n",
    "    Possible actions to chose from are: 0=left; 1=right\n",
    "    Observations are floats indicating the current field index, e.g. 0.0 for\n",
    "    starting position, 1.0 for the field next to the starting position, etc..\n",
    "    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = gym.spaces.Discrete(2)  # left and right\n",
    "        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\"\"\"\n",
    "        self.cur_pos = 0\n",
    "        # Return initial observation.\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given `action`\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, done-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "        # Walk left.\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Walk right.\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        # Set `done` flag when end of corridor (goal) reached.\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # +1 when goal reached, otherwise -1.\n",
    "        reward = 1.0 if done else -0.1\n",
    "        return [self.cur_pos], reward, done, {}\n",
    "\n",
    "\n",
    "# Create an RLlib Trainer instance.\n",
    "trainer = PPOTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": SimpleCorridor,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "            # Use corridor with 20 fields (including S and G).\n",
    "            \"corridor_length\": 20\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to move at least 19 times in the env to reach the goal and\n",
    "# each move gives us -0.1 reward (except the last move at the end: +1.0),\n",
    "# we can expect to reach an optimal episode reward of -0.1*18 + 1.0 = -0.8\n",
    "for i in range(5):\n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly different env here (len 10 instead of 20),\n",
    "# however, this should still work as the agent has (hopefully) learned\n",
    "# to \"just always walk right!\"\n",
    "env = SimpleCorridor({\"corridor_length\": 10})\n",
    "# Get the initial observation (should be: [0.0] for the starting position).\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = trainer.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "# Report results.\n",
    "print(f\"Played 1 episode; total-reward={total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4cb3386-425a-4485-ac7e-fe1f28bc718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3710000000000009\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    env = SimpleCorridor({\"corridor_length\": 20})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # Compute a single action, given the current observation\n",
    "        # from the environment.\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        # Apply the computed action in the environment.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Sum up rewards for reporting purposes.\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            rewardList.append(total_reward)\n",
    "        \n",
    "    # Report results.\n",
    "#    print(f\"Played 1 episode; total-reward={total_reward}\")\n",
    "print(np.mean(rewardList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5320b1-1c53-4842-a842-dbb7f42ceacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263d554-17d4-4c1d-8c6f-1863106c8330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
