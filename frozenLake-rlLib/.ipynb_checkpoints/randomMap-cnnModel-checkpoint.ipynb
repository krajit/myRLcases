{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97311a8-4f83-4fd6-bff4-de4bb7272f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa918f6-1bfb-490b-99c4-4846fef657e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedFrozenLake(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.old = 0\n",
    "        self.desc = self.env.desc\n",
    "        self.size = len(self.desc)\n",
    "        self.observation_space = gym.spaces.Box(0,1,shape=(3,self.size,self.size,),dtype=\"float32\")\n",
    "        self.action_space = self.env.action_space\n",
    "        \n",
    "    def oneHot(self,s):\n",
    "        x = np.zeros(self.size*self.size)\n",
    "        x[s] = 1\n",
    "        state_ = np.array([ x.reshape(self.size,self.size),\n",
    "                         np.array(self.env.desc == b\"F\").astype(\"float32\"),\n",
    "                         np.array(self.env.desc == b\"G\").astype(\"float32\")\n",
    "                          ])\n",
    "        return state_.reshape(3,self.size,self.size,)\n",
    "\n",
    "    def reset(self):\n",
    "        # return self.oneHot(1)\n",
    "        return self.oneHot(self.env.reset())\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, r, done, info = self.env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            reward = 2 if r > 0 else -1\n",
    "        elif obs == self.old:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.old = obs\n",
    "        return self.oneHot(obs), reward, done, info\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config): \n",
    "    size = env_config['size']\n",
    "    numHoles = env_config['numHoles']\n",
    "    p = 1-numHoles/(size**2)\n",
    "    desc = generate_random_map(size=size, p=p)\n",
    "    return WrappedFrozenLake(gym.make('FrozenLake-v1', desc = desc, is_slippery=False))  # return an env instance\n",
    "\n",
    "register_env(\"myenv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFF\n",
      "FFFFFF\n",
      "HFFFFF\n",
      "F\u001b[41mH\u001b[0mHFFF\n",
      "FFFFFH\n",
      "HFFFFG\n"
     ]
    }
   ],
   "source": [
    "env = env_creator(env_config = {'size':6, 'numHoles': 6})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ded3d8-452a-430e-9669-d510ea8d1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = torch.device(\"cpu\")#\"cuda\"\n",
    "        #                            if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.mainLayer = nn.Sequential(\n",
    "                # nn.Conv2d(3,12,kernel_size=3,stride= 1,padding=1),\n",
    "                # nn.ReLU(),\n",
    "                # nn.Conv2d(12,24,kernel_size=3,stride=1,padding=1),\n",
    "                # nn.ReLU(),\n",
    "                # nn.Conv2d(24,36,kernel_size=3,stride=1,padding=1),\n",
    "                # nn.ReLU(),            \n",
    "                nn.Flatten(start_dim=-3,end_dim=-1),\n",
    "                nn.Linear(108,256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,256),\n",
    "                nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(256, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(256, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.mainLayer(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(-1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89613814-2ead-496a-a4be-fa2f2942fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0319,  0.0357,  0.1205, -0.0275], grad_fn=<AddBackward0>), [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 6\n",
    "test_model_torch = MyTorchModel(\n",
    "   obs_space=gym.spaces.Box(0,1,shape=(3,size,size), dtype=np.float32),\n",
    "   action_space=gym.spaces.Discrete(4),\n",
    "   num_outputs=4,\n",
    "   model_config={},\n",
    "   name=\"MyModel\",\n",
    ")\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n",
    "\n",
    "obs = gym.spaces.Box(0,1,shape=(3,size,size)).sample()\n",
    "test_model_torch({\"obs\": torch.from_numpy(obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d875ff98-1145-45ce-b343-cd36683bc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"framework\": \"torch\",\n",
    "        \"env\":\"myenv\",  \n",
    "        \"env_config\":{'size':6, 'numHoles': 6},\n",
    "#        \"num_workers\": 6,\n",
    "          \"model\": {\n",
    "             \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "             \"custom_model_config\": {},\n",
    "          },\n",
    " #       'num_envs_per_worker': 1,\n",
    "        \"create_env_on_driver\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d15567-ca54-4b3c-9a8c-385c6cbe743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "tune_config = config.copy()\n",
    "tune_config[\"lr\"] = tune.grid_search([0.0001])  # <- 0.5? again: ouch!\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": 50,\n",
    "    \"episode_reward_mean\": 1.95,\n",
    "}\n",
    "\n",
    "analysis =  tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "    checkpoint_at_end=True,  \n",
    "    checkpoint_freq=5,  \n",
    "    local_dir=\"checkPoints/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba06c8a-bf47-41c2-837d-d43356e2255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7f34f88079a0>\n",
      "Found best checkpoint for trial #2: /home/ajit.kumar@SNU.IN/Desktop/myRLcases/frozenLake-rlLib/checkPoints/PPO/PPO_myenv_5306c_00000_0_lr=0.0001_2022-07-16_14-27-30/checkpoint_000018/checkpoint-18\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis)\n",
    "# Get all trials (we only have one).\n",
    "trials = analysis.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf9ff5d9-896a-4229-a3e3-b33e093b0bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 14:30:52,599\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-07-16 14:30:52,600\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-07-16 14:30:52,716\tINFO trainable.py:588 -- Restored on 10.13.62.8 from checkpoint: /home/ajit.kumar@SNU.IN/Desktop/myRLcases/frozenLake-rlLib/checkPoints/PPO/PPO_myenv_5306c_00000_0_lr=0.0001_2022-07-16_14-27-30/checkpoint_000018/checkpoint-18\n",
      "2022-07-16 14:30:52,717\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 18, '_timesteps_total': None, '_time_total': 86.2672049999237, '_episodes_total': 7718}\n"
     ]
    }
   ],
   "source": [
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.0001\n",
    "#rllib_config[\"train_batch_size\"] = 4159\n",
    "#rllib_config[\"explore\"] = False\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63a14e-27bb-4b3a-8e7c-625e867b4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93a3da2-3f53-4b3e-95d0-7ac37caf9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.46\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "    env = env_creator({'size':6, 'numHoles': 6}) #gym.make(\"myenv-v1\")#\"FrozenLake-v1\")\n",
    "    #, #FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "#        display.clear_output(wait=True)\n",
    "        # print(\"num won: \", won, \" played: \", i, \"total reward: \", total_reward)\n",
    "        # env.render()\n",
    "\n",
    "        action = new_trainer.compute_single_action(obs,explore=False)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #print(obs, reward, done)\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "        if done: \n",
    "            rewardList.append(total_reward)\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "      \n",
    "    \n",
    "#print(rewardList, np.mean(rewardList))\n",
    "print( np.mean(rewardList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e4b27fd-5647-4b1a-b350-e05b328baf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFHFFF\n",
      "FF\u001b[41mH\u001b[0mFFF\n",
      "FHFFFF\n",
      "FFHHFF\n",
      "FFFHFF\n",
      "HFFFFG\n"
     ]
    }
   ],
   "source": [
    "env = env_creator({'size':6, 'numHoles': 6})# gym.make(\"FrozenLake-v1\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    display.clear_output(wait=True)\n",
    "    obs, reward, done, _ = env.step(new_trainer.compute_single_action(obs,explore=False))\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    if done: \n",
    "        rewardList.append(total_reward)\n",
    "if reward > 0 :\n",
    "    won +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce34032-d04a-48f8-bf67-42d92cd39621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
