{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d70bee-8a31-4d67-afff-25e144eb9b54",
   "metadata": {},
   "source": [
    "# TODO: Add CNN Model\n",
    "\n",
    "# TODO Try with ray tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97311a8-4f83-4fd6-bff4-de4bb7272f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa918f6-1bfb-490b-99c4-4846fef657e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedFrozenLake(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.old = 0\n",
    "        self.desc = self.env.desc\n",
    "        self.size = len(self.desc)\n",
    "        self.observation_space = gym.spaces.Box(0,1,shape=(3,self.size,self.size,),dtype=\"float32\")\n",
    "        self.action_space = self.env.action_space\n",
    "        \n",
    "    def oneHot(self,s):\n",
    "        x = np.zeros(self.size*self.size)\n",
    "        x[s] = 1\n",
    "        state_ = np.array([ x.reshape(self.size,self.size),\n",
    "                         np.array(self.env.desc == b\"F\").astype(\"float32\"),\n",
    "                         np.array(self.env.desc == b\"G\").astype(\"float32\")\n",
    "                          ])\n",
    "        return state_.reshape(3,self.size,self.size,)\n",
    "\n",
    "    def reset(self):\n",
    "        # return self.oneHot(1)\n",
    "        return self.oneHot(self.env.reset())\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, r, done, info = self.env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            reward = 2 if r > 0 else -1\n",
    "        elif obs == self.old:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.old = obs\n",
    "        return self.oneHot(obs), reward, done, info\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config): \n",
    "    size = env_config['size']\n",
    "    numHoles = env_config['numHoles']\n",
    "    p = 1-numHoles/(size**2)\n",
    "    desc = generate_random_map(size=size, p=p)\n",
    "    return WrappedFrozenLake(gym.make('FrozenLake-v1', desc = desc, is_slippery=False))  # return an env instance\n",
    "\n",
    "register_env(\"myenv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFF\n",
      "FFFFFF\n",
      "FFFFFF\n",
      "FFFFFF\n",
      "FHFFF\u001b[41mH\u001b[0m\n",
      "FFFFFG\n"
     ]
    }
   ],
   "source": [
    "env = env_creator(env_config = {'size':6, 'numHoles': 6})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ded3d8-452a-430e-9669-d510ea8d1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = torch.device(\"cpu\")#\"cuda\"\n",
    "        #                            if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.mainLayer = nn.Sequential(\n",
    "                # nn.Conv2d(3,12,kernel_size=3,stride= 1,padding=1),\n",
    "                # nn.ReLU(),\n",
    "                # nn.Conv2d(12,24,kernel_size=3,stride=1,padding=1),\n",
    "                # nn.ReLU(),\n",
    "                # nn.Conv2d(24,36,kernel_size=3,stride=1,padding=1),\n",
    "                # nn.ReLU(),            \n",
    "                nn.Flatten(start_dim=-3,end_dim=-1),\n",
    "                nn.Linear(108,256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256,256),\n",
    "                nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(256, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(256, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.mainLayer(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(-1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89613814-2ead-496a-a4be-fa2f2942fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0119, -0.0290,  0.1312,  0.0470], grad_fn=<AddBackward0>), [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 6\n",
    "test_model_torch = MyTorchModel(\n",
    "   obs_space=gym.spaces.Box(0,1,shape=(3,size,size,), dtype=np.int32),\n",
    "   action_space=gym.spaces.Discrete(4),\n",
    "   num_outputs=4,\n",
    "   model_config={},\n",
    "   name=\"MyModel\",\n",
    ")\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n",
    "\n",
    "obs = gym.spaces.Box(0,1,shape=(3,size,size)).sample()\n",
    "test_model_torch({\"obs\": torch.from_numpy(obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d875ff98-1145-45ce-b343-cd36683bc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"framework\": \"torch\",\n",
    "        \"env\":\"myenv\",  \n",
    "        \"env_config\":{'size':6, 'numHoles': 6},\n",
    "        \"num_workers\": 6,\n",
    "          \"model\": {\n",
    "            # \"fcnet_hiddens\": [512, 1024, 256],\n",
    "            # \"fcnet_activation\": \"relu\",\n",
    "             \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "             \"custom_model_config\": {},\n",
    "          },\n",
    "#        \"num_workers\": 1,\n",
    "        'num_envs_per_worker': 1,\n",
    "        # 'gamma': 1.0,\n",
    "        # 'lr': 0.001,\n",
    "        # 'train_batch_size': 1000,  \n",
    "        \"create_env_on_driver\": True,\n",
    "}\n",
    "\n",
    "trainer = DQNTrainer(config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a086bc-804f-450c-b4ec-2ab42b2c5532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fceeafc-f55a-4d33-9459-dc92a3b476bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21addd5d-a56e-4755-aecf-4bc2301558f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1; avg. reward=1.4\n",
      "Iter: 2; avg. reward=1.32\n",
      "Iter: 3; avg. reward=1.05\n",
      "Iter: 4; avg. reward=0.7\n",
      "Iter: 5; avg. reward=0.74\n",
      "Iter: 6; avg. reward=0.5\n",
      "Iter: 7; avg. reward=0.62\n",
      "Iter: 8; avg. reward=0.91\n",
      "Iter: 9; avg. reward=1.29\n",
      "Iter: 10; avg. reward=1.23\n",
      "Iter: 11; avg. reward=1.17\n",
      "Iter: 12; avg. reward=1.22\n",
      "Iter: 13; avg. reward=1.62\n",
      "Iter: 14; avg. reward=1.76\n",
      "Iter: 15; avg. reward=1.81\n",
      "Iter: 16; avg. reward=1.89\n",
      "Iter: 17; avg. reward=1.69\n",
      "Iter: 18; avg. reward=1.55\n",
      "Iter: 19; avg. reward=1.39\n",
      "Iter: 20; avg. reward=1.57\n",
      "Iter: 21; avg. reward=1.5\n",
      "Iter: 22; avg. reward=1.58\n",
      "Iter: 23; avg. reward=1.63\n",
      "Iter: 24; avg. reward=1.51\n",
      "Iter: 25; avg. reward=1.4\n",
      "Iter: 26; avg. reward=1.24\n",
      "Iter: 27; avg. reward=1.38\n",
      "Iter: 28; avg. reward=1.6\n",
      "Iter: 29; avg. reward=1.62\n",
      "Iter: 30; avg. reward=1.45\n",
      "Iter: 31; avg. reward=1.29\n",
      "Iter: 32; avg. reward=1.43\n",
      "Iter: 33; avg. reward=0.79\n",
      "Iter: 34; avg. reward=0.79\n",
      "Iter: 35; avg. reward=0.71\n",
      "Iter: 36; avg. reward=1.52\n",
      "Iter: 37; avg. reward=1.58\n",
      "Iter: 38; avg. reward=1.67\n",
      "Iter: 39; avg. reward=1.72\n",
      "Iter: 40; avg. reward=1.66\n",
      "Iter: 41; avg. reward=1.46\n",
      "Iter: 42; avg. reward=1.67\n",
      "Iter: 43; avg. reward=1.5\n",
      "Iter: 44; avg. reward=1.47\n",
      "Iter: 45; avg. reward=1.36\n",
      "Iter: 46; avg. reward=1.61\n",
      "Iter: 47; avg. reward=1.63\n",
      "Iter: 48; avg. reward=1.77\n",
      "Iter: 49; avg. reward=1.81\n",
      "Iter: 50; avg. reward=1.64\n"
     ]
    }
   ],
   "source": [
    "keepTraining = True\n",
    "maxIter = 50\n",
    "i = 0\n",
    "\n",
    "while keepTraining and i < (maxIter):  \n",
    "    i += 1 \n",
    "    results = trainer.train()\n",
    "    if results['episode_reward_mean'] > 1.99:\n",
    "        keepTraining = False\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659419a5-2d52-49cd-a63e-2591d7e299da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 100 was saved in '/home/ajit.kumar@SNU.IN/ray_results/DQNTrainer_myenv_2022-07-16_13-11-35ybmiq79b/checkpoint_000100/checkpoint-100'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-100.tune_metadata', 'checkpoint-100', '.is_checkpoint']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = trainer.save()\n",
    "print(f\"Trainer (at iteration {trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2326e71f-1507-4460-b23d-0584fea4b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 13:18:08,604\tWARNING ppo.py:386 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 666.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loaded state dict contains a parameter group that doesn't match the size of optimizer's group",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#eval_config['explore'] = False\u001b[39;00m\n\u001b[1;32m      3\u001b[0m eval_trainer2 \u001b[38;5;241m=\u001b[39m PPOTrainer(config\u001b[38;5;241m=\u001b[39meval_config)\n\u001b[0;32m----> 4\u001b[0m \u001b[43meval_trainer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m eval_trainer2\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating new trainer: R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/tune/trainable.py:583\u001b[0m, in \u001b[0;36mTrainable.restore\u001b[0;34m(self, checkpoint_path, checkpoint_node_ip)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(checkpoint_dict)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps_since_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:2070\u001b[0m, in \u001b[0;36mTrainer.load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;129m@override\u001b[39m(Trainable)\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     extra_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(checkpoint_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 2070\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextra_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   2691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state:\n\u001b[0;32m-> 2693\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2694\u001b[0m         remote_state \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mput(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers():\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:1542\u001b[0m, in \u001b[0;36mRolloutWorker.restore\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_policy(\n\u001b[1;32m   1535\u001b[0m             policy_id\u001b[38;5;241m=\u001b[39mpid,\n\u001b[1;32m   1536\u001b[0m             policy_cls\u001b[38;5;241m=\u001b[39mpol_spec\u001b[38;5;241m.\u001b[39mpolicy_class,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m             config\u001b[38;5;241m=\u001b[39mpol_spec\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   1540\u001b[0m         )\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1542\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py:278\u001b[0m, in \u001b[0;36mPPOTorchPolicy.set_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_coeff \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_kl_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Call super's set_state with rest of the state dict.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py:767\u001b[0m, in \u001b[0;36mTorchPolicy.set_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers, optimizer_vars):\n\u001b[1;32m    766\u001b[0m         optim_state_dict \u001b[38;5;241m=\u001b[39m convert_to_torch_tensor(s, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 767\u001b[0m         \u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptim_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# Set exploration's state.\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexploration\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exploration_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state:\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/torch/optim/optimizer.py:146\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    144\u001b[0m saved_lens \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m saved_groups)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(p_len \u001b[38;5;241m!=\u001b[39m s_len \u001b[38;5;28;01mfor\u001b[39;00m p_len, s_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(param_lens, saved_lens)):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded state dict contains a parameter group \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the size of optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Update the state\u001b[39;00m\n\u001b[1;32m    150\u001b[0m id_map \u001b[38;5;241m=\u001b[39m {old_id: p \u001b[38;5;28;01mfor\u001b[39;00m old_id, p \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    151\u001b[0m           \u001b[38;5;28mzip\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable((g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m saved_groups)),\n\u001b[1;32m    152\u001b[0m               chain\u001b[38;5;241m.\u001b[39mfrom_iterable((g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups)))}\n",
      "\u001b[0;31mValueError\u001b[0m: loaded state dict contains a parameter group that doesn't match the size of optimizer's group"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +7m27s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +7m27s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    }
   ],
   "source": [
    "eval_config = config.copy()\n",
    "#eval_config['explore'] = False\n",
    "eval_trainer2 = PPOTrainer(config=eval_config)\n",
    "eval_trainer2.restore(checkpoint_file)\n",
    "\n",
    "results = eval_trainer2.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e50ae9-adcf-4e48-a25f-5a055da16b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-2.2215700660308144\n"
     ]
    }
   ],
   "source": [
    "results = eval_trainer2.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3367f5bc-8895-4312-bebd-71c0de6387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_checkpoint('model/checkpoint_000050/checkpoint-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91461ee2-8111-491e-97d1-9c7469a6e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "    env = env_creator({'size':6, 'numHoles': 6}) #gym.make(\"myenv-v1\")#\"FrozenLake-v1\")\n",
    "    #, #FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "#        display.clear_output(wait=True)\n",
    "        # print(\"num won: \", won, \" played: \", i, \"total reward: \", total_reward)\n",
    "        # env.render()\n",
    "\n",
    "        action = new_trainer.compute_single_action(obs,explore=False)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #print(obs, reward, done)\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "        if done: \n",
    "            rewardList.append(total_reward)\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "      \n",
    "    \n",
    "#print(rewardList, np.mean(rewardList))\n",
    "print( np.mean(rewardList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec0a2fed-4274-44a9-b75a-a382ca577ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a19b681-2ca3-4648-a9c8-26f59f45404f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_single_action(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81b51076-950e-4d67-99b6-e8ce1751a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFF\n",
      "FFFHFF\n",
      "FF\u001b[41mH\u001b[0mFFF\n",
      "FFFFFF\n",
      "FFFFHF\n",
      "FFFHFG\n"
     ]
    }
   ],
   "source": [
    "env = env_creator({'size':6, 'numHoles': 6})# gym.make(\"FrozenLake-v1\")\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    display.clear_output(wait=True)\n",
    "    obs, reward, done, _ = env.step(new_trainer.compute_single_action(obs,explore=False))\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    if done: \n",
    "        rewardList.append(total_reward)\n",
    "if reward > 0 :\n",
    "    won +=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56383a75-ac4d-492e-ae2e-b3838bc1f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.stop()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d15567-ca54-4b3c-9a8c-385c6cbe743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "# How about we try two different learning rates? Let's say 0.00005 and 0.5 (ouch!).\n",
    "tune_config[\"lr\"] = tune.grid_search([0.0001])  # <- 0.5? again: ouch!\n",
    "#tune_config[\"train_batch_size\"] = tune.grid_search([4000])\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 50,\n",
    "    \"episode_reward_mean\": 1.95,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "analysis =  tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=1,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba06c8a-bf47-41c2-837d-d43356e2255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7fab282ceaa0>\n",
      "Found best checkpoint for trial #2: /home/ajit.kumar@SNU.IN/ray_results/PPO/PPO_myenv_4159a_00000_0_lr=0.0001_2022-07-16_12-46-48/checkpoint_000017/checkpoint-17\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis)\n",
    "# Get all trials (we only have one).\n",
    "trials = analysis.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf9ff5d9-896a-4229-a3e3-b33e093b0bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 12:59:09,211\tWARNING ppo.py:386 -- `train_batch_size` (4159) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 693.\n",
      "2022-07-16 12:59:09,329\tINFO trainable.py:588 -- Restored on 10.13.62.8 from checkpoint: /home/ajit.kumar@SNU.IN/ray_results/PPO/PPO_myenv_4159a_00000_0_lr=0.0001_2022-07-16_12-46-48/checkpoint_000017/checkpoint-17\n",
      "2022-07-16 12:59:09,330\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 17, '_timesteps_total': None, '_time_total': 122.87536454200745, '_episodes_total': 16946}\n"
     ]
    }
   ],
   "source": [
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.0001\n",
    "rllib_config[\"train_batch_size\"] = 4159\n",
    "#rllib_config[\"explore\"] = False\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63a14e-27bb-4b3a-8e7c-625e867b4fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18dfa64b-216c-4e4a-8e16-6ecbd43e30a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO_myenv_4159a_00000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07236cef-0d7e-4f61-8547-7bb1e40ccab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95098963-4ddd-4cc4-a1a4-fb376480c1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a3da2-3f53-4b3e-95d0-7ac37caf9264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
