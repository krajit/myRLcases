{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae92c19-99c3-4047-942a-015086300eb6",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [ ] Code ran with flattened observation. How to run with nd-array observation? \n",
    "- [ ] How to provide pytorch net that I trained earlier for fronzen lake.\n",
    "- [ ] Use ray tuner for hyperparamter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434e2086-47d6-415f-84d8-030ab0d89ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c30796f-3fe8-4d3e-9021-72d5c3662f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redifne this map to put starting and ending point in random location\n",
    "# TODO: Can we do this with decorators?\n",
    "def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    Args:\n",
    "        size: size of each side of the grid\n",
    "        p: probability that a tile is frozen\n",
    "    Returns:\n",
    "        A random valid map\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res,sx,sy):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((sx, sy))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        sx = np.random.randint(size); sy = np.random.randint(size)\n",
    "        res[sx][sy] = \"S\"\n",
    "        gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        while res[gx][gy] == \"S\": # we don't want to overwrite the S\n",
    "            gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        res[gx][gy] = \"G\"   \n",
    "        valid = is_valid(res,sx,sy)\n",
    "    return [\"\".join(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb5573b1-d96d-446c-a52a-119703605d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeWrapped(gym.Env):\n",
    "    def __init__(self,config):\n",
    "        self.size = 6 # grid size\n",
    "        nH = 5.    # desired number of hole: this is probabilistic\n",
    "        pH = 1 - nH/(self.size**2) # probability of  grid being a hole\n",
    "        desc = generate_random_map(size= self.size, p= pH)\n",
    "        self.env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n",
    "        self.observation_space = gym.spaces.Box(0,1,shape=(3*self.size*self.size,), dtype=np.int32)\n",
    "        self.action_space = self.env.action_space\n",
    "    def oneHot(self,s):\n",
    "        x = np.zeros(self.size*self.size)\n",
    "        x[s] = 1\n",
    "        state_ = np.array([ x.reshape(self.size,self.size),\n",
    "                         np.array(self.env.desc == b\"F\").astype(\"int32\"),\n",
    "                         np.array(self.env.desc == b\"G\").astype(\"int32\")\n",
    "                          ])\n",
    "        return state_.flatten()\n",
    "    \n",
    "    def reset(self):\n",
    "        o = self.env.reset()\n",
    "        return self.oneHot(o)\n",
    "\n",
    "    def step(self, action):\n",
    "        o, r, done, info = self.env.step(action)\n",
    "        return self.oneHot(o), r, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "FFHSFF\n",
      "FFF\u001b[41mH\u001b[0mFH\n",
      "FFGFFF\n",
      "FFHFFF\n",
      "FFFFFF\n",
      "FFFFFF\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeWrapped({})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d8fae5e-4b53-4692-aa6a-7c7c2e23f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=28380)\u001b[0m 2022-07-08 13:59:43,233\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28381)\u001b[0m 2022-07-08 13:59:43,228\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28379)\u001b[0m 2022-07-08 13:59:43,284\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28378)\u001b[0m 2022-07-08 13:59:43,278\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(\n",
    "    config={\n",
    "        \"env\": FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 4,\n",
    "        \n",
    "            \"model\": {\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21addd5d-a56e-4755-aecf-4bc2301558f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=0.33903133903133903\n",
      "Iter: 1; avg. reward=0.4988009592326139\n",
      "Iter: 2; avg. reward=0.6858168761220825\n",
      "Iter: 3; avg. reward=0.7934936350777935\n",
      "Iter: 4; avg. reward=0.8304696449026346\n",
      "Iter: 5; avg. reward=0.9101851851851852\n",
      "Iter: 6; avg. reward=0.9418493803622497\n",
      "Iter: 7; avg. reward=0.9677152317880795\n",
      "Iter: 8; avg. reward=0.9870722433460076\n",
      "Iter: 9; avg. reward=0.9970501474926253\n",
      "Iter: 10; avg. reward=0.9979064898813678\n",
      "Iter: 11; avg. reward=0.9973579920739762\n",
      "Iter: 12; avg. reward=0.9980569948186528\n",
      "Iter: 13; avg. reward=1.0\n",
      "Iter: 14; avg. reward=0.9993906154783668\n",
      "Iter: 15; avg. reward=0.9993975903614458\n",
      "Iter: 16; avg. reward=0.9994044073853484\n",
      "Iter: 17; avg. reward=1.0\n",
      "Iter: 18; avg. reward=0.998211091234347\n",
      "Iter: 19; avg. reward=1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91461ee2-8111-491e-97d1-9c7469a6e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num won:  2  played:  19\n",
      "  (Right)\n",
      "FFFFFF\n",
      "FFFFFF\n",
      "FFFFFF\n",
      "HFFHHF\n",
      "FFS\u001b[41mF\u001b[0mGF\n",
      "FFFFHF\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    env = FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"num won: \", won, \" played: \", i)\n",
    "        env.render()\n",
    "\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c0a43-e141-4d8a-8e02-88ba1e2b3f86",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
