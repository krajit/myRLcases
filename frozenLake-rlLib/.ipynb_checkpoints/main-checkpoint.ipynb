{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae92c19-99c3-4047-942a-015086300eb6",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] Code ran with flattened observation. How to run with nd-array observation? \n",
    "- [x] How to provide pytorch net that I trained earlier for fronzen lake.\n",
    "- [ ] Use ray tuner for hyperparamter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434e2086-47d6-415f-84d8-030ab0d89ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from typing import List, Optional\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c30796f-3fe8-4d3e-9021-72d5c3662f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redifne this map to put starting and ending point in random location\n",
    "# TODO: Can we do this with decorators?\n",
    "def newGenerate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    Args:\n",
    "        size: size of each side of the grid\n",
    "        p: probability that a tile is frozen\n",
    "    Returns:\n",
    "        A random valid map\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res,sx,sy):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((sx, sy))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        sx = np.random.randint(size); sy = np.random.randint(size)\n",
    "        res[sx][sy] = \"S\"\n",
    "        gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        while res[gx][gy] == \"S\": # we don't want to overwrite the S\n",
    "            gx = np.random.randint(size); gy = np.random.randint(size)\n",
    "        res[gx][gy] = \"G\"   \n",
    "        valid = is_valid(res,sx,sy)\n",
    "    return [\"\".join(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5573b1-d96d-446c-a52a-119703605d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeWrapped(gym.Env):\n",
    "    def __init__(self,config):\n",
    "        self.size = 6 # grid size\n",
    "        nH = 10.    # desired number of hole: this is probabilistic\n",
    "        pH = 1 - nH/(self.size**2) # probability of  grid being a hole\n",
    "        desc = newGenerate_random_map(size= self.size, p= pH)\n",
    "        self.env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n",
    "        self.observation_space = gym.spaces.Box(-1.0,2.9,shape=(3,self.size,self.size,))\n",
    "        self.action_space = self.env.action_space\n",
    "    def oneHot(self,s):\n",
    "        x = np.zeros(self.size*self.size)\n",
    "        x[s] = 1\n",
    "        state_ = np.array([ x.reshape(self.size,self.size),\n",
    "                         np.array(self.env.desc == b\"F\").astype(\"float32\"),\n",
    "                         np.array(self.env.desc == b\"G\").astype(\"float32\")\n",
    "                          ])\n",
    "        return state_ + np.random.rand(state_.size).reshape(state_.shape)/10.\n",
    "    \n",
    "    def reset(self):\n",
    "        o = self.env.reset()\n",
    "        return self.oneHot(o)\n",
    "\n",
    "    def step(self, action):\n",
    "        o, r, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            if r > 0:\n",
    "                newReward = 10\n",
    "            else:\n",
    "                newReward = -2\n",
    "        else:\n",
    "            newReward = -1\n",
    "            \n",
    "        return self.oneHot(o), newReward , done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13337bb-777c-4c97-a40a-bc2a643d2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "HHFFHH\n",
      "GFFFFF\n",
      "FFFHHF\n",
      "FFHFFS\n",
      "FF\u001b[41mH\u001b[0mFFF\n",
      "FFFFFH\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeWrapped({})\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # plt.cla()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.1)\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090757ae-b518-4ab3-b94b-6d2a3a67d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = torch.device(\"cpu\")#\"cuda\"\n",
    "        #                            if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.mainLayer = nn.Sequential(\n",
    "                nn.Conv2d(3,12,kernel_size=3,stride= 1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(12,24,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(24,36,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Flatten(start_dim=-3,end_dim=-1),\n",
    "                nn.Linear(1296,1200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1200,64),\n",
    "                nn.ReLU(),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(64, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(64, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.mainLayer(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(-1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85977dc6-d028-45b9-a264-ef60d69a523e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0328], grad_fn=<AddBackward0>), [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 6\n",
    "test_model_torch = MyTorchModel(\n",
    "   obs_space=gym.spaces.Box(0,1,shape=(3,size,size,), dtype=np.int32),\n",
    "   action_space=gym.spaces.Discrete(4),\n",
    "   num_outputs=1,\n",
    "   model_config={},\n",
    "   name=\"MyModel\",\n",
    ")\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n",
    "\n",
    "obs = gym.spaces.Box(0,1,shape=(3,size,size)).sample()\n",
    "test_model_torch({\"obs\": torch.from_numpy(obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8fae5e-4b53-4692-aa6a-7c7c2e23f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m 2022-07-11 15:16:19,040\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m 2022-07-11 15:16:19,035\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m 2022-07-11 15:16:19,002\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m 2022-07-11 15:16:19,029\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1065931, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f78e72e5930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065931)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m 2022-07-11 15:16:19,054\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1065934, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7faad1d09930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065934)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m 2022-07-11 15:16:19,053\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1065933, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb14c505960>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065933)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m 2022-07-11 15:16:19,111\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m 2022-07-11 15:16:19,129\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1065932, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f432d019930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1065932)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1065931, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f78e72e5930>)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n    self[policy_id] = class_(observation_space, action_space, merged_config)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n    self.model, dist_class = make_model_and_action_dist(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n    model = ModelCatalog.get_model_v2(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n    return wrapper(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n    super(DQNTorchModel, self).__init__(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n    model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n    raise ValueError(\nValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDQNTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mFrozenLakeWrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframework\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfcnet_hiddens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfcnet_activation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"model\": {\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"custom_model_config\": {\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     #\"layers\": [128, 128],\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# },\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_envs_per_worker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync_function_tpl\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers_for_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:142\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     local_worker\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m ):\n\u001b[0;32m--> 142\u001b[0m     remote_spaces \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    148\u001b[0m         e[\u001b[38;5;241m0\u001b[39m]: (\u001b[38;5;28mgetattr\u001b[39m(e[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, e[\u001b[38;5;241m1\u001b[39m]), e[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m remote_spaces\n\u001b[1;32m    150\u001b[0m     }\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/worker.py:1833\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1833\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   1836\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1065931, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f78e72e5930>)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n    self[policy_id] = class_(observation_space, action_space, merged_config)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n    self.model, dist_class = make_model_and_action_dist(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n    model = ModelCatalog.get_model_v2(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n    return wrapper(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n    super(DQNTorchModel, self).__init__(\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n    model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n  File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n    raise ValueError(\nValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m 2022-07-11 15:16:21,696\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m 2022-07-11 15:16:21,733\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066043, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f1c85f51930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066043)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m 2022-07-11 15:16:21,759\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m 2022-07-11 15:16:21,776\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066044, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7efcb2a41930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066044)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m 2022-07-11 15:16:21,988\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m 2022-07-11 15:16:22,006\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066042, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7a1eac1930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066042)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m 2022-07-11 15:16:22,017\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m 2022-07-11 15:16:22,034\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066048, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7b94f9d930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066048)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m 2022-07-11 15:16:24,559\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m 2022-07-11 15:16:24,577\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066158, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4790f0d8d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066158)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m 2022-07-11 15:16:24,648\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m 2022-07-11 15:16:24,666\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066161, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ef5982d1930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066161)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m 2022-07-11 15:16:24,725\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m 2022-07-11 15:16:24,742\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066165, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f2ff1425930>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066165)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m 2022-07-11 15:16:24,784\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m 2022-07-11 15:16:24,802\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1066167, ip=10.13.62.8, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f16c0d718d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 152, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     self[policy_id] = class_(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 278, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\", line 180, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     model = ModelCatalog.get_model_v2(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 733, in get_model_v2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     return wrapper(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/agents/dqn/dqn_torch_model.py\", line 61, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     super(DQNTorchModel, self).__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/torch/visionnet.py\", line 33, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     model_config[\"conv_filters\"] = get_filter_config(obs_space.shape)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m   File \"/home/ajit.kumar@SNU.IN/anaconda3/envs/udemy-rl/lib/python3.10/site-packages/ray/rllib/models/utils.py\", line 128, in get_filter_config\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1066167)\u001b[0m ValueError: No default configuration for obs shape [3, 6, 6], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n"
     ]
    }
   ],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "            # \"model\": {\n",
    "            # \"fcnet_hiddens\": [256, 800, 256],\n",
    "            # \"fcnet_activation\": \"relu\",\n",
    "        \"model\": {\n",
    "        \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "        },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a38af8-e1d2-4d8f-8d8f-ffa22ed6d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        \"env\": FrozenLakeWrapped,\n",
    "        \"framework\": \"torch\",\n",
    "        \"env_config\": {},\n",
    "        \"num_workers\": 12,\n",
    "        \n",
    "            \"model\": {\n",
    "            \"fcnet_hiddens\": [256, 800, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        # \"model\": {\n",
    "        # \"custom_model\": MyTorchModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        # \"custom_model_config\": {\n",
    "        #     #\"layers\": [128, 128],\n",
    "        # },\n",
    "        },\n",
    "    'num_envs_per_worker': 10,\n",
    "    'gamma': 0.90,\n",
    "    'lr': 0.001,\n",
    "    'train_batch_size': 200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21addd5d-a56e-4755-aecf-4bc2301558f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,613\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,622\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,629\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,637\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,646\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,653\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,561\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,646\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,651\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,552\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,557\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,569\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,576\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,584\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,590\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,597\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,604\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063587)\u001b[0m 2022-07-11 15:14:22,610\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,553\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,641\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,649\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,575\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,582\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,590\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,598\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,608\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,616\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,624\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,631\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063586)\u001b[0m 2022-07-11 15:14:22,639\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,568\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,630\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,635\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,643\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,651\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,656\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,656\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,661\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,669\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063584)\u001b[0m 2022-07-11 15:14:22,676\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,662\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,667\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,672\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,676\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,680\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063585)\u001b[0m 2022-07-11 15:14:22,685\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,664\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,673\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,680\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,687\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,694\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063588)\u001b[0m 2022-07-11 15:14:22,701\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,661\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,669\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,677\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,681\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063589)\u001b[0m 2022-07-11 15:14:22,686\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,662\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,746\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,750\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,754\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,758\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,763\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,748\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,767\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,771\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,775\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063583)\u001b[0m 2022-07-11 15:14:22,780\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-07-11 15:14:22,874\tWARNING deprecation.py:46 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `RepayBuffer.add()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,801\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,806\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,810\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,815\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,820\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,824\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,828\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,832\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1063590)\u001b[0m 2022-07-11 15:14:22,836\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-07-11 15:14:22,904\tWARNING multi_agent_prioritized_replay_buffer.py:186 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "2022-07-11 15:14:22,916\tWARNING deprecation.py:46 -- DeprecationWarning: `replay` has been deprecated. Use `sample` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-2.5258493353028064\n",
      "Iter: 1; avg. reward=-3.19941348973607\n",
      "Iter: 2; avg. reward=-3.6082621082621085\n",
      "Iter: 3; avg. reward=-2.626126126126126\n",
      "Iter: 4; avg. reward=-3.9725490196078432\n",
      "Iter: 5; avg. reward=-0.6703296703296703\n",
      "Iter: 6; avg. reward=-3.7565789473684212\n",
      "Iter: 7; avg. reward=-3.0536756126021003\n",
      "Iter: 8; avg. reward=-3.178733031674208\n",
      "Iter: 9; avg. reward=-2.455056179775281\n",
      "Iter: 10; avg. reward=-2.9771490750816105\n",
      "Iter: 11; avg. reward=-0.5974477958236659\n",
      "Iter: 12; avg. reward=-3.0978520286396183\n",
      "Iter: 13; avg. reward=-3.4049295774647885\n",
      "Iter: 14; avg. reward=-2.50375\n",
      "Iter: 15; avg. reward=-2.2853982300884956\n",
      "Iter: 16; avg. reward=-2.1703617269544924\n",
      "Iter: 17; avg. reward=-4.218867924528302\n",
      "Iter: 18; avg. reward=-2.67512077294686\n",
      "Iter: 19; avg. reward=-2.0214205186020293\n",
      "Iter: 20; avg. reward=-0.5327963176064442\n",
      "Iter: 21; avg. reward=-1.2796709753231492\n",
      "Iter: 22; avg. reward=-3.8293785310734463\n",
      "Iter: 23; avg. reward=-2.894141829393628\n",
      "Iter: 24; avg. reward=-1.7001689189189189\n",
      "Iter: 25; avg. reward=-1.2960069444444444\n",
      "Iter: 26; avg. reward=-1.7830882352941178\n",
      "Iter: 27; avg. reward=-2.820048309178744\n",
      "Iter: 28; avg. reward=-1.0560747663551402\n",
      "Iter: 29; avg. reward=-1.9332659251769464\n",
      "Iter: 30; avg. reward=-2.108617594254937\n",
      "Iter: 31; avg. reward=-1.3285568065506652\n",
      "Iter: 32; avg. reward=-1.473953013278856\n",
      "Iter: 33; avg. reward=-1.5260791366906474\n",
      "Iter: 34; avg. reward=-3.121107266435986\n",
      "Iter: 35; avg. reward=-1.6580976863753214\n",
      "Iter: 36; avg. reward=-1.348773841961853\n",
      "Iter: 37; avg. reward=-0.9442176870748299\n",
      "Iter: 38; avg. reward=-1.035650623885918\n",
      "Iter: 39; avg. reward=-3.1404886561954624\n",
      "Iter: 40; avg. reward=-1.6347305389221556\n",
      "Iter: 41; avg. reward=-1.0684161199625117\n",
      "Iter: 42; avg. reward=-1.0317948717948717\n",
      "Iter: 43; avg. reward=-2.075924075924076\n",
      "Iter: 44; avg. reward=-1.3998250218722659\n",
      "Iter: 45; avg. reward=-0.5916453537936914\n",
      "Iter: 46; avg. reward=-0.9297205757832345\n",
      "Iter: 47; avg. reward=-1.403448275862069\n",
      "Iter: 48; avg. reward=-0.7338247338247338\n",
      "Iter: 49; avg. reward=-0.4437716262975779\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "659419a5-2d52-49cd-a63e-2591d7e299da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/checkpoint_000300/checkpoint-300'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3367f5bc-8895-4312-bebd-71c0de6387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load_checkpoint(\"model/checkpoint_000300/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91461ee2-8111-491e-97d1-9c7469a6e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.55\n"
     ]
    }
   ],
   "source": [
    "won = 0\n",
    "lost = 0\n",
    "rewardList = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    env = FrozenLakeWrapped({})\n",
    "    # Get the initial observation (should be: [0.0] for the starting position).\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    # Play one episode.\n",
    "    while not done:\n",
    "        # plt.cla()\n",
    "#        display.clear_output(wait=True)\n",
    "        # print(\"num won: \", won, \" played: \", i, \"total reward: \", total_reward)\n",
    "        # env.render()\n",
    "\n",
    "        action = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # display.display(plt.gcf())\n",
    "        # plt.gcf()\n",
    "    if reward > 0 :\n",
    "        won +=1 \n",
    "    rewardList.append(total_reward)\n",
    "    \n",
    "#print(rewardList, np.mean(rewardList))\n",
    "print( np.mean(rewardList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2db7732f-6a45-43c2-b799-c23e2e6b45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Why is testing average mean reward is very low compared to training mean reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b51076-950e-4d67-99b6-e8ce1751a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-0.8885918003565062\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):    \n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724d42d-71a2-47d9-9548-e777ce0af1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
