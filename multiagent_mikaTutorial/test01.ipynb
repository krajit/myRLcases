{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8034ddc9",
   "metadata": {},
   "source": [
    "## A simple Multi-Agent RL notebook.\n",
    "The green agent is trying to swipe the entire grid, while the blue agent is trying to stop the green agent from doing it.\n",
    "\n",
    "![fdfd](ezgif.com-gif-maker.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9eb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's code our multi-agent environment.\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        # Dimensions of the grid.\n",
    "        self.width = config.get(\"width\", 10)\n",
    "        self.height = config.get(\"height\", 10)\n",
    "\n",
    "        # End an episode after this many timesteps.\n",
    "        self.timestep_limit = config.get(\"ts\", 100)\n",
    "\n",
    "        self.observation_space = MultiDiscrete([self.width * self.height,\n",
    "                                                self.width * self.height])\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Reset env.\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Returns initial observation of next(!) episode.\"\"\"\n",
    "        # Row-major coords.\n",
    "        self.agent1_pos = [0, 0]  # upper left corner\n",
    "        self.agent2_pos = [self.height - 1, self.width - 1]  # lower bottom corner\n",
    "\n",
    "        # Accumulated rewards in this episode.\n",
    "        self.agent1_R = 0.0\n",
    "        self.agent2_R = 0.0\n",
    "\n",
    "        # Reset agent1's visited fields.\n",
    "        self.agent1_visited_fields = set([tuple(self.agent1_pos)])\n",
    "\n",
    "        # How many timesteps have we done in this episode.\n",
    "        self.timesteps = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "        \n",
    "        # increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "            \n",
    "        # Get observations (based on new agent positions).\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        \n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        return obs, rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"new\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    # def render(self, mode=None):\n",
    "    #     print(\"_\" * (self.width + 2))\n",
    "    #     for r in range(self.height):\n",
    "    #         print(\"|\", end=\"\")\n",
    "    #         for c in range(self.width):\n",
    "    #             field = r * self.width + c % self.width\n",
    "    #             if self.agent1_pos == [r, c]:\n",
    "    #                 print(\"1\", end=\"\")\n",
    "    #             elif self.agent2_pos == [r, c]:\n",
    "    #                 print(\"2\", end=\"\")\n",
    "    #             elif (r, c) in self.agent1_visited_fields:\n",
    "    #                 print(\".\", end=\"\")\n",
    "    #             else:\n",
    "    #                 print(\" \", end=\"\")\n",
    "    #         print(\"|\")\n",
    "    #     print(\"â€¾\" * (self.width + 2))\n",
    "    #     print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "    #     print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "    #     print(\"R2={: .1f}\".format(self.agent2_R))\n",
    "    #     print()\n",
    "\n",
    "    def render(self, mode=\"rgb_array\"):\n",
    "            sqSize = 20\n",
    "            canvasSizeW = sqSize*self.width+1\n",
    "            canvasSizeH = sqSize*self.height+1\n",
    "\n",
    "            canvas = pygame.Surface((canvasSizeW, canvasSizeH))\n",
    "            canvas.fill((255, 255, 255))\n",
    "            for i in range(self.width+2):\n",
    "                pygame.draw.line(canvas, 0, (0, sqSize*i), (canvasSizeW, sqSize*i), width=1)\n",
    "            for i in range(self.height+2):\n",
    "                pygame.draw.line(canvas, 0, ( sqSize*i,0), (sqSize*i,canvasSizeH, ), width=1)\n",
    "            \n",
    "            greenColor = (0,255,0)\n",
    "            visitedColor = (0,100,0)\n",
    "            blueColor = (0,0,255)\n",
    "\n",
    "            for (i,j) in self.agent1_visited_fields:\n",
    "                pygame.draw.rect(canvas, visitedColor, pygame.Rect(sqSize*(i)+1, sqSize*(j)+1, sqSize-1, sqSize-1))\n",
    "\n",
    "            i, j = self.agent1_pos \n",
    "            pygame.draw.rect(canvas, greenColor, pygame.Rect(sqSize*(i)+1, sqSize*(j)+1, sqSize-1, sqSize-1))\n",
    "\n",
    "            i, j = self.agent2_pos \n",
    "            pygame.draw.circle(canvas, blueColor, (sqSize*(i)+int(sqSize/2), sqSize*(j)+int(sqSize/2)), int(sqSize/2))\n",
    "\n",
    "\n",
    "            plArray = np.array(pygame.surfarray.pixels3d(canvas))\n",
    "            plt.imshow(plArray)        \n",
    "            plt.axis(\"off\")\n",
    "\n",
    "\n",
    "env = MultiAgentArena()\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Agent1 will move down, Agent2 moves up.\n",
    "obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077c7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[2, 0]\n",
      "Agent2's x/y position=[7, 9]\n",
      "Env timesteps=2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQWklEQVR4nO3ce2icdb4G8Oc3M5nJNI1NZlKr9JY0aQvWaqnQQCkra5Hzh+4uuDFelmUPLeylK02LlaVrXLBowQRsVcq6dMv6h9CKImURFaVaEzEtNtAbZGJzsUlrM5PLpLm9c3vnd/4Y6vGwmcyk5xv26znPR/KHvjPP/Hybp2FC8hhrLYhIH8+/+wBENDuWk0gplpNIKZaTSCmWk0gp31wXjTH8Vi7RArPWmtn++5zlBAD8GoBX8CRRoPabWly+fFkwFHj66aexZcsW7NmzRzR3yZIl6O3tRVVVlVhmd3c3tm/fjmvXrollAsBzzz0HYwxaWlpEc5cvX47PP/8c69atE8scHh7G2rVrMT4+LpYJAIcOHUJnZyfefvtt0dwNGzbg6NGj2Lp1q1im4zgIhUJ5rxcup7eoRxXPCxhjUFpaKhgKeL1e+Hw+8VwACAQCormBQAAAFuQeeDye/9f3wOfzwev1iucaY+D3+0VzC/2MAd9zEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKVV4gCQK2QqP5bZT2traBENzmzR9fX3iudZadHR0oKKiQixzYGAA6XRa/KzXr1+HMUY8N5PJ4Ouvv0Y0GhXLjMfjyGaz4mft6+vD8PCweK7jOLhw4QKy2axYZjKZnPO6mWvHxBhjcYfYWXJcoCRVgurqatHYGzduoLS0FJWVlaK5PT09qKmpgdcrt3KWSqUwODiI2tpasUwg9xcUACxdulQ0t7e3FytXroTf7xfLdF0X/f39qKurE8sEcqVPJBK4++67RXO//fZbLFu2DMFgUCzTWouenp6863uFy/mfkB34GgLqInW4cuWKYCjw+OOPo76+Hvv27RPNDQQCuHbtmugnfCQSwbZt2zAyMiKWCQB79uyBx+PBq6++KpobDofx1VdfYf369WKZsVgMq1atQiKREMsEgNbWVpw7dw7vvPOOaG5dXR3eeustbNu2TSzTcRwsWrQobzn5npNIKZaTSCmWk0gplpNIKZaTSCmWk0gplpNIKZaTSCmWk0gplpNIKZaTSCmWk0gplpNIKZaTSCmWk0gplpNIKZaTSCmWk0gplpNIqcLrQJ8DmHXh5DYlgBuTN9DQ0CAYCpw5cwbd3d04c+aMaG4mk8GOHTsQCATEMicnJzE5OSl+Dy5cuAAgt+4naWpqCrt370Z5eblYZjKZRDqdFr8H33zzDeLxuHju0NAQmpubUVVVJZbpuu6c1wuXc1dRjypeHxB8J4itW7cKhgJXrlzB6tWrxXNPnjyJLVu2oKysTCwzFovhyy+/FD/rrfU96dyPPvoImzdvFh05m5qawscffyx+1mQyCY/HI557+vRpbNy4ETU1NWKZ6XQaJ0+ezP8Aa23eDwAWDqzoP22wdXV1VlpDQ4NtbW0Vz/X7/TYWi4lmdnV12XA4LJpprbVNTU1279694rmhUMhGIhHRzGg0agOBgGimtda2tLTYxsZG8dza2lrb3t4umjkzM2NzFZy9f3zPSaQUy0mkFMtJpBTLSaQUy0mkFMtJpBTLSaQUy0mkFMtJpBTLSaQUy0mkFMtJpBTLSaQUy0mkFMtJpBTLSaQUy0mkFMtJpBTLSaSUsbmtoNkvGmPhF35FCyAN+P2ywZlMBsYYeL1e0dxUKiV+Vmst0um0eO6tNbeFuAclJSUwRnKGcWHureu6sNbC55NcpVvYe2CtnTW04P/BYO+g6CzkmTNn0NTUhLNnz4plAsDOnTvxwAMPYNeuXaK5K1aswIULFxAOh8Uye3p68OijjyISiYhlAkBzczM8Hg8OHDggmrt+/Xp8+OGHqK2tFcscGRnBpk2bcO3aNbFMADhy5AjOnz+Po0ePiuZu2bIFb7zxBurr68UyE4kEVq1alfd6wXJWVVWhtLRU7EAVFRXwer2iM4sAEAgEUFZWJp4LAOFwWDR3dHQUxhjxswaDQXg8HvFcYwwqKytFc621C3IPysrKEAgExHO9Xi8qKipEcx3HmfM633MSKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpVXCmpLu7W3RDaGBgAKlUSnw/Z3JyEsPDw+K51lr09PRgdHRULLOvrw+u64qfNR6PwxgjnpvNZtHX14e5xuDma2xsDNZa8bPGYjFMTEyI56ZSKQwMDIjmJpPJOa8XXN+THLYCcit5k5OTqKysFM2dnJyEz+dDMBgUzR0dHUUoFBJdXXNdFzdv3kQoFBLLBIDp6WkAuR0dSWNjY1iyZInoqp+1FmNjY6LDaUBulyeTyaC8vFw0Nx6Po7y8XHzVb3R0NO/6Hqy1eT8AWMdxrKS2tjZbV1cnmmmttQ0NDba1tVU81+/321gsJprZ1dVlw+GwaKa11jY1Ndm9e/eK54ZCIRuJREQzo9GoDQQCopnWWtvS0mIbGxvFc2tra217e7to5szMjM1VcPb+8T0nkVIsJ5FSLCeRUiwnkVIsJ5FSLCeRUiwnkVIsJ5FSLCeRUiwnkVIsJ5FSLCeRUiwnkVIsJ5FSLCeRUiwnkVIsJ5FSLCeRUiwnkVIF14qeffZZ0VGj7777DsPDw2hqahLLBIDz589jaGgIg4ODormu6+L5558XHQ6Lx+OYnp4WvwdtbW0wxojnzszM4OWXXxYdZbs1xCV91s7OTsRiMfHckZERvPbaa3j33XfFMjOZzJzXC7bO6/WKrq55PB4YY0QzAcAYA4/HI54LLMw9uJUryRizIPcW+PHcg4X6/ALk74EtNDWab/nLcn3PWsv1PWu5vmct1/eI6AdYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqWMnWNkyBhjKysrYYwRe8F0Oo3p6WlUVFSIZQLA1NQUvF6v6EoeAIyNjaGiouL7QSoJrutiYmJCdM0OAKanp2GMwaJFi0Rz4/E47rjjDtFxq2w2i/HxcYRCIbFMILfq57ouFi9eLJo7Pj6OsrIylJSUiGVaaxGPx2GtnbVgBct58eJFBAIBsQOdO3cO+/fvx6effiqWCQBNTU24//77sWPHDtHce++9F+3t7aJF6uvrw1NPPYWzZ8+KZQLAwYMHYYzB/v37RXPr6+tx4sQJ1NTUiGWOjY3hwQcfxKVLl8QyAeDYsWO4dOkSDh8+LJr78MMP45VXXsHmzZvFMpPJJO6777685Sw4jbl27VqUlpaKHSgajcLv92PdunVimQCwePFiVFVViecaY7BmzRosXbpULDObzcLr9Yqf9dZXeOlcj8eD6upq0dxYLAZjjPhZq6qqUF5eLp5bUlKCFStWiOY6jjPndb7nJFKK5SRSiuUkUorlJFKK5SRSiuUkUorlJFKK5SRSiuUkUorlJFKK5SRSiuUkUorlJFKK5SRSiuUkUorlJFKK5SRSiuUkUqrgTEksFhPdEIrH43BdF9FoVCwTyO2xTE1NiecCwPDwMLLZrFje6OgorLXiZ3UcB8YY8VxrLUZHR0VzF+oeTE1NIZFIiOe6rot4PJ4n1wugKs8z4wBSs15JJBJzvmbBgS/JYgK5P+h0Og2/3y+am06n4fF4RBfigFzpF+IepFIp8dxMJgMA8PkK/p07L8lkEn6/X3SF8Vau9D1wXRfZbFZ0JQ8AUqkUSkpK/sc9yGaBdBoAlgPozfPM/wDwBXw+YLZPzWQymXfgC9bavB8ArOM4VlJbW5utq6sTzbTW2oaGBtva2iqe6/f7bSwWE83s6uqy4XBYNNNaa5uamuzevXvFc0OhkI1EIqKZ0WjUBgIB0UxrrW1pabGNjY3iubW1tba9vf37f89mrf3nP60Fivs4cCD3nB+amZmxuQrO3j++5yS6DX/9K/Dznxf/+L/8Bfjtb+f3Giwn0Tw1NwN79sz/ef/4B/CznxX/eJaTaB5+/3vg8OFb7zXnx3WBTz4BfvrT4h7PchIV6Te/AU6cAKanbz8jlQI6OoorqOy39Yj+D+vrk8lJJoGursKP41dOoqK8BKBOLG1iAvjjH+d+DL9yEhXlSdE0x8l9g2gu/MpJpBTLSaQUy0mkFMtJpBTLSVSUFAC530wCgEI/m89yEhXlHgBfiaUtWwbcvDn3Y1hOIqVYTqIiffIJ8Mtf/u9z7rkH6M33658/wHISFSkYBP72N+APf7j9jJ/8BDh1CigrK/xYlpNoHsJh4M9/Bv70p/k/9xe/AP7+d+Cuu4p7PH98j2ieVqwAfve73HdbX3qpuOf86lfAvn3A2rXFvw7LSXQbamqAnTv/+7uuzc2zP2737lwh6+uBTZvm9xoFy3no0CHRwaj+/n7E43G0traKZQJAd3c3HMe5tX0kxnVdHDlyBGXFvEkoUiwWg+M44vegs7MTxhjx3EQigWPHjmHp0qVimVNTU3BdV/ysX3zxBQYHB8Vzx8fHcfz4cXR0dPzLNcfJ/7xkMnf99Oncxw+lC/zGdsHWdXZ2ii7aDQ8PI5lM4ty5c2KZQG5y0+PxiOdaa3H+/HnRlbiJiQlkMhnxs0ajURhjxHMzmQwuX76M8vJyscxEIoFsNit+1sHBQcTjcfHcRCKBSCSCkZGRWa83Ns7+vHgcyHcU13XnftF8y1+W63vWWq7vWcv1PWv/dX1PAtf3iH6kWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKlWE4ipVhOIqVYTiKljJ1jrc4YY9esWQNjjNgLOo6DWCyG1atXi2UCwNDQEEpLS1FRUSGa29vbi+rqatGRs1QqhevXr6OmpkYsE8D341NVVVWiuf39/Vi+fDn8fr9Ypuu6uHr1KtasWSOWCeRW8hKJBO4qdrm5SFevXsWdd96JYDAolpnNZtHf3w9r7awFK1jOU6dOif6hXLx4EQcPHsSJEyfEMgHghRdewIYNG/Dkk0+K5j700EN4//33RUs/MDCAXbt24YMPPhDLBIDXX38dHo8HzzzzjGjuI488gjfffBMrV64UyxwfH8djjz2Gzz77TCwTAI4fP45IJIIXX3xRNPeJJ55Ac3MzNm7cKJaZSqWwffv2vOUsOI25detWlJaWih3IWotgMIht27aJZQK5rxbV1dXiucYY1NfXi262RiIR+Hw+8bO+99578Hg84rk+nw+bN2/G+vXrxTJjsdiCnLWjowMjIyPiucFgEBs3bhTNdeYavAXfcxKpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEinFchIpxXISKcVyEilVcKbEcRxks1mxF0wmk8hms5iZmRHLBHKDUel0WjwXyN0DydxEIgEA4mfNZDLweDwLcg8SicSP4h6k02lkMhnx3Gw2i2QyuSD3IJ+CA19iJyGiWd3W+h4R/fvwPSeRUiwnkVIsJ5FSLCeRUiwnkVIsJ5FS/wW3dsAHiM5pCAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agent1 will move down, Agent2 moves up.\n",
    "obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_agent1=1\n",
      "action_agent2=0\n",
      "\n",
      "action_agent1=1\n",
      "action_agent2=0\n",
      "\n",
      "action_agent1=0\n",
      "action_agent2=0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DummyTrainer:\n",
    "    \"\"\"Dummy Trainer class used in Exercise #1.\n",
    "\n",
    "    Use its `compute_action` method to get a new action for one of the agents,\n",
    "    given the agent's observation (a single discrete value encoding the field\n",
    "    the agent is currently in).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_action(self, single_agent_obs=None):\n",
    "        # Returns a random action for a single agent.\n",
    "        return np.random.randint(4)  # Discrete(4) -> return rand int between 0 and 3 (incl. 3).\n",
    "\n",
    "dummy_trainer = DummyTrainer()\n",
    "# Check, whether it's working.\n",
    "for _ in range(3):\n",
    "    # Get action for agent1 (providing agent1's and agent2's positions).\n",
    "    print(\"action_agent1={}\".format(dummy_trainer.compute_action(np.array([0, 99]))))\n",
    "\n",
    "    # Get action for agent2 (providing agent2's and agent1's positions).\n",
    "    print(\"action_agent2={}\".format(dummy_trainer.compute_action(np.array([99, 0]))))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8a1d-7d82-4b79-b2d4-3ce7ffa6fae1",
   "metadata": {},
   "source": [
    "Write your solution code into this cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b12373b-4b71-4ee9-a7a1-13077a59840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543b5b7977d74767b3dcf129024d982e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.\n",
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Solution to Exercise #1:\n",
    "\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env.\n",
    "    env.reset()\n",
    "    \n",
    "    # 2) Enter an infinite while loop (to step through the episode).\n",
    "    while True:\n",
    "        # 3) Calculate both agents' actions individually, using dummy_trainer.compute_action([individual agent's obs])\n",
    "        a1 = dummy_trainer.compute_action(obs[\"agent1\"])\n",
    "        a2 = dummy_trainer.compute_action(obs[\"agent2\"])\n",
    "        # 4) Compile the actions dict from both individual agents' actions.\n",
    "        actions = {\"agent1\": a1, \"agent2\": a2}\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts\n",
    "        obs, rewards, done, infos = env.step(actions)\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True)\n",
    "        \n",
    "        time.sleep(0.08)\n",
    "        env.render()\n",
    "        plt.show()\n",
    "\n",
    "        # 7) Check, whether the episde is done, if yes, break out of the while loop.\n",
    "        if done[\"__all__\"]:\n",
    "            break\n",
    "\n",
    "# 8) Run it! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd830b90-5762-4d22-8fa9-0abf0777a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:32:52,436\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.12', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '10.13.62.8', 'raylet_ip_address': '10.13.62.8', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-10-03_12-24-15_047385_3254446/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-10-03_12-24-15_047385_3254446/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-10-03_12-24-15_047385_3254446', 'metrics_export_port': 61387, 'gcs_address': '10.13.62.8:44367', 'address': '10.13.62.8:44367', 'node_id': '524a421e0c1430906989b0281fc58968fed46f72bc1c7934fcce498b'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "ray.init(ignore_reinit_error=True)  # Hear the engine humming? ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bcc1116-a14c-4479-87c0-6ece58ab0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:33:15,225\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-10-03 12:33:15,227\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-10-03 12:33:15,228\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-10-03 12:33:15,322\tWARNING env.py:216 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOTrainer"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256193)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.9.12)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256193)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256192)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.9.12)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256192)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256192)\u001b[0m 2022-10-03 12:33:18,722\tWARNING env.py:216 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,  # \"my_env\" <- if we previously have registered the env with `tune.register_env(\"[name]\", lambda config: [returns env object])`.\n",
    "    \"env_config\": {\n",
    "        \"config\": {\n",
    "            \"width\": 10,\n",
    "            \"height\": 10,\n",
    "            \"ts\": 100,\n",
    "        },\n",
    "    },\n",
    "    \"create_env_on_driver\": True,\n",
    "}\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f6c94d4-6871-4d20-81af-3d4081f05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:33:20,910\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 4000,\n",
      " 'counters': {'num_agent_steps_sampled': 4000,\n",
      "              'num_agent_steps_trained': 4000,\n",
      "              'num_env_steps_sampled': 4000,\n",
      "              'num_env_steps_trained': 4000},\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-10-03_12-33-25',\n",
      " 'done': False,\n",
      " 'episode_len_mean': 100.0,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': 17.999999999999986,\n",
      " 'episode_reward_mean': -8.325,\n",
      " 'episode_reward_min': -39.00000000000006,\n",
      " 'episodes_this_iter': 20,\n",
      " 'episodes_total': 20,\n",
      " 'experiment_id': 'ffe6fc15c88e4abd89cef6931a1455ee',\n",
      " 'hist_stats': {'episode_lengths': [100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100,\n",
      "                                    100],\n",
      "                'episode_reward': [-9.899999999999979,\n",
      "                                   -39.00000000000006,\n",
      "                                   17.999999999999986,\n",
      "                                   -3.899999999999985,\n",
      "                                   -21.900000000000034,\n",
      "                                   -11.999999999999993,\n",
      "                                   -13.799999999999978,\n",
      "                                   -2.999999999999978,\n",
      "                                   -6.299999999999981,\n",
      "                                   -4.799999999999992,\n",
      "                                   12.000000000000009,\n",
      "                                   -33.00000000000003,\n",
      "                                   -22.500000000000014,\n",
      "                                   -13.499999999999979,\n",
      "                                   6.600000000000021,\n",
      "                                   -28.80000000000005,\n",
      "                                   14.999999999999982,\n",
      "                                   3.0000000000000204,\n",
      "                                   -15.29999999999997,\n",
      "                                   6.600000000000032]},\n",
      " 'hostname': 'mathlab-Precision-5820-Tower',\n",
      " 'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
      "                                         'learner_stats': {'cur_kl_coeff': 0.20000000298023224,\n",
      "                                                           'cur_lr': 4.999999873689376e-05,\n",
      "                                                           'entropy': 1.3757896,\n",
      "                                                           'entropy_coeff': 0.0,\n",
      "                                                           'kl': 0.010701312,\n",
      "                                                           'model': {},\n",
      "                                                           'policy_loss': -0.024049541,\n",
      "                                                           'total_loss': 5.9703884,\n",
      "                                                           'vf_explained_var': 0.025619099,\n",
      "                                                           'vf_loss': 5.992298},\n",
      "                                         'num_agent_steps_trained': 128.0}},\n",
      "          'num_agent_steps_sampled': 4000,\n",
      "          'num_agent_steps_trained': 4000,\n",
      "          'num_env_steps_sampled': 4000,\n",
      "          'num_env_steps_trained': 4000},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '10.13.62.8',\n",
      " 'num_agent_steps_sampled': 4000,\n",
      " 'num_agent_steps_trained': 4000,\n",
      " 'num_env_steps_sampled': 4000,\n",
      " 'num_env_steps_sampled_this_iter': 4000,\n",
      " 'num_env_steps_trained': 4000,\n",
      " 'num_env_steps_trained_this_iter': 4000,\n",
      " 'num_healthy_workers': 2,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 36.15833333333333,\n",
      "          'gpu_util_percent0': 0.7250000000000001,\n",
      "          'ram_util_percent': 30.78333333333333,\n",
      "          'vram_util_percent0': 0.6216851569266301},\n",
      " 'pid': 3254446,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {'mean_action_processing_ms': 0.10844746550599058,\n",
      "                  'mean_env_render_ms': 0.0,\n",
      "                  'mean_env_wait_ms': 0.06532454705023981,\n",
      "                  'mean_inference_ms': 0.970083635884684,\n",
      "                  'mean_raw_obs_processing_ms': 0.26359960630342566},\n",
      " 'sampler_results': {'custom_metrics': {},\n",
      "                     'episode_len_mean': 100.0,\n",
      "                     'episode_media': {},\n",
      "                     'episode_reward_max': 17.999999999999986,\n",
      "                     'episode_reward_mean': -8.325,\n",
      "                     'episode_reward_min': -39.00000000000006,\n",
      "                     'episodes_this_iter': 20,\n",
      "                     'hist_stats': {'episode_lengths': [100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100,\n",
      "                                                        100],\n",
      "                                    'episode_reward': [-9.899999999999979,\n",
      "                                                       -39.00000000000006,\n",
      "                                                       17.999999999999986,\n",
      "                                                       -3.899999999999985,\n",
      "                                                       -21.900000000000034,\n",
      "                                                       -11.999999999999993,\n",
      "                                                       -13.799999999999978,\n",
      "                                                       -2.999999999999978,\n",
      "                                                       -6.299999999999981,\n",
      "                                                       -4.799999999999992,\n",
      "                                                       12.000000000000009,\n",
      "                                                       -33.00000000000003,\n",
      "                                                       -22.500000000000014,\n",
      "                                                       -13.499999999999979,\n",
      "                                                       6.600000000000021,\n",
      "                                                       -28.80000000000005,\n",
      "                                                       14.999999999999982,\n",
      "                                                       3.0000000000000204,\n",
      "                                                       -15.29999999999997,\n",
      "                                                       6.600000000000032]},\n",
      "                     'off_policy_estimator': {},\n",
      "                     'policy_reward_max': {},\n",
      "                     'policy_reward_mean': {},\n",
      "                     'policy_reward_min': {},\n",
      "                     'sampler_perf': {'mean_action_processing_ms': 0.10844746550599058,\n",
      "                                      'mean_env_render_ms': 0.0,\n",
      "                                      'mean_env_wait_ms': 0.06532454705023981,\n",
      "                                      'mean_inference_ms': 0.970083635884684,\n",
      "                                      'mean_raw_obs_processing_ms': 0.26359960630342566}},\n",
      " 'time_since_restore': 5.773890495300293,\n",
      " 'time_this_iter_s': 5.773890495300293,\n",
      " 'time_total_s': 5.773890495300293,\n",
      " 'timers': {'learn_throughput': 960.308,\n",
      "            'learn_time_ms': 4165.33,\n",
      "            'load_throughput': 6999255.736,\n",
      "            'load_time_ms': 0.571,\n",
      "            'training_iteration_time_ms': 5764.204,\n",
      "            'update_time_ms': 2.629},\n",
      " 'timestamp': 1664780605,\n",
      " 'timesteps_since_restore': 0,\n",
      " 'timesteps_total': 4000,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default',\n",
      " 'warmup_time': 4.091665029525757}\n"
     ]
    }
   ],
   "source": [
    "results = rllib_trainer.train()\n",
    "del results[\"config\"]\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dff7017-f1b9-41e8-94fd-266bbe56cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_env_on_driver': True,\n",
      " 'env': <class '__main__.MultiAgentArena'>,\n",
      " 'env_config': {'config': {'height': 10, 'ts': 100, 'width': 10}},\n",
      " 'multiagent': {'policies': {'policy1': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {}),\n",
      "                             'policy2': (None,\n",
      "                                         MultiDiscrete([100 100]),\n",
      "                                         Discrete(4),\n",
      "                                         {'lr': 0.0002})},\n",
      "                'policy_mapping_fn': <function policy_mapping_fn at 0x7fdd78227670>}}\n",
      "\n",
      "agent1 is now mapped to policy1\n",
      "agent2 is now mapped to policy2\n"
     ]
    }
   ],
   "source": [
    "policies = {\n",
    "    \"policy1\": (None, env.observation_space, env.action_space, {}),\n",
    "    \"policy2\": (None, env.observation_space, env.action_space, {\"lr\": 0.0002}),\n",
    "}\n",
    "def policy_mapping_fn(agent_id: str):\n",
    "    assert agent_id in [\"agent1\", \"agent2\"], f\"ERROR: invalid agent ID {agent_id}!\"\n",
    "    return \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "config.update({\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "})\n",
    "\n",
    "pprint.pprint(config)\n",
    "print()\n",
    "print(f\"agent1 is now mapped to {policy_mapping_fn('agent1')}\")\n",
    "print(f\"agent2 is now mapped to {policy_mapping_fn('agent2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "646f8800-941b-43cb-a924-622af6788aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOTrainer"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256325)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.9.12)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256325)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256326)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.9.12)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256326)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256325)\u001b[0m 2022-10-03 12:33:37,849\tWARNING env.py:216 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# Recreate our Trainer (we cannot just change the config on-the-fly).\n",
    "rllib_trainer.stop()\n",
    "\n",
    "# Using our updated (now multiagent!) config dict.\n",
    "rllib_trainer = PPOTrainer(config=config)\n",
    "rllib_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "Now that we are setup correctly with two policies as per our \"multiagent\" config, let's call `train()` on the new Trainer several times (what about 10 times?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17ae724d-71cc-422b-96cb-3dc9faa2d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3256325)\u001b[0m 2022-10-03 12:33:39,194\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=-10.664999999999997\n",
      "Iteration=2: R(\"return\")=-5.516249999999994\n",
      "Iteration=3: R(\"return\")=-2.7179999999999915\n",
      "Iteration=4: R(\"return\")=0.31200000000001005\n",
      "Iteration=5: R(\"return\")=1.6830000000000067\n",
      "Iteration=6: R(\"return\")=1.5990000000000066\n",
      "Iteration=7: R(\"return\")=2.3730000000000064\n",
      "Iteration=8: R(\"return\")=3.4830000000000036\n",
      "Iteration=9: R(\"return\")=5.714999999999998\n",
      "Iteration=10: R(\"return\")=7.0199999999999925\n"
     ]
    }
   ],
   "source": [
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "# Move on once you see (agent1 + agent2) episode rewards of 10.0 or more.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "365ef0d7-9977-4d9d-9fa5-ffaa7c111b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=11: R(\"return\")=11.115000000000009 R1=17.65 R2=-6.534999999999989\n",
      "Iteration=12: R(\"return\")=13.104000000000012 R1=19.265 R2=-6.160999999999989\n",
      "Iteration=13: R(\"return\")=13.737000000000013 R1=19.92 R2=-6.182999999999989\n",
      "Iteration=14: R(\"return\")=14.973000000000013 R1=20.815 R2=-5.841999999999988\n",
      "Iteration=15: R(\"return\")=16.581000000000014 R1=22.885 R2=-6.303999999999989\n",
      "Iteration=16: R(\"return\")=18.92100000000001 R1=25.83 R2=-6.908999999999987\n",
      "Iteration=17: R(\"return\")=20.325000000000014 R1=26.42 R2=-6.094999999999988\n",
      "Iteration=18: R(\"return\")=21.243000000000013 R1=26.37 R2=-5.126999999999989\n",
      "Iteration=19: R(\"return\")=21.24900000000001 R1=23.23 R2=-1.9809999999999914\n",
      "Iteration=20: R(\"return\")=20.550000000000008 R1=20.54 R2=0.010000000000008875\n"
     ]
    }
   ],
   "source": [
    "# Do another loop, but this time, we will print out each policies' individual rewards.\n",
    "for _ in range(10):\n",
    "    results = rllib_trainer.train()\n",
    "    r1 = results['policy_reward_mean']['policy1']\n",
    "    r2 = results['policy_reward_mean']['policy2']\n",
    "    r = r1 + r2\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={r} R1={r1} R2={r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 20 was saved in '/home/ajit/ray_results/PPOTrainer_MultiAgentArena_2022-10-03_12-33-34w91p_kvc/checkpoint_000020/checkpoint-20'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-20.tune_metadata', 'checkpoint-20', '.is_checkpoint']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = rllib_trainer.save()\n",
    "print(f\"Trainer (at iteration {rllib_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "import os\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:39:05,194\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3257636)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.9.12)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3257636)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3257635)\u001b[0m pygame 2.1.2 (SDL 2.0.16, Python 3.9.12)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3257635)\u001b[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3257635)\u001b[0m 2022-10-03 12:39:07,348\tWARNING env.py:216 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2022-10-03 12:39:11,304\tINFO trainable.py:588 -- Restored on 10.13.62.8 from checkpoint: /home/ajit/ray_results/PPOTrainer_MultiAgentArena_2022-10-03_12-33-34w91p_kvc/checkpoint_000020/checkpoint-20\n",
      "2022-10-03 12:39:11,306\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 265.8637044429779, '_episodes_total': 800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new trainer: R=-8.054999999999998\n",
      "Before restoring: Trainer is at iteration=0\n",
      "After restoring: Trainer is at iteration=20\n",
      "Evaluating restored trainer: R=21.19499999999993\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_trainer = PPOTrainer(config=config)\n",
    "# Evaluate the new trainer (this should yield random results).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating new trainer: R={results['evaluation']['episode_reward_mean']}\")\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "new_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_trainer.iteration}\")\n",
    "\n",
    "# Evaluate again (this should yield results we saw after having trained our saved agent).\n",
    "results = new_trainer.evaluate()\n",
    "print(f\"Evaluating restored trainer: R={results['evaluation']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "245839b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc3722c7a22437e8b057f5c0de37952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        a1 = new_trainer.compute_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "        a2 = new_trainer.compute_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "        actions = {\"agent1\": a1, \"agent2\": a2}\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        plt.show()\n",
    "        time.sleep(0.07)\n",
    "\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2976125)\u001b[0m E0930 11:32:59.527818979 2976219 chttp2_transport.cc:1103]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2976129)\u001b[0m E0930 11:32:59.528018199 2976259 chttp2_transport.cc:1103]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2976127)\u001b[0m E0930 11:32:59.527847016 2976240 chttp2_transport.cc:1103]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n"
     ]
    }
   ],
   "source": [
    "rllib_trainer.stop()\n",
    "new_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c1e4c-cb02-4719-ac5a-0106172a6c6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Moving stuff to the professional level: RLlib in connection w/ Ray Tune\n",
    "\n",
    "Running any experiments through Ray Tune is the recommended way of doing things with RLlib. If you look at our\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/rllib/examples\">examples scripts folder</a>, you will see that almost all of the scripts use Ray Tune to run the particular RLlib workload demonstrated in each script.\n",
    "\n",
    "<img src=\"images/rllib_and_tune.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdacebb-d27f-4174-9002-35c5657f146c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When setting up hyperparameter sweeps for Tune, we'll do this in our already familiar config dict.\n",
    "\n",
    "So let's take a quick look at our PPO algo's default config to understand, which hyperparameters we may want to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1b32582-52bd-4585-9009-2f877a0723a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': True,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'disable_env_checking': False,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': None,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'recreate_failed_workers': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts and Ray Tune.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# PPO algorithm:\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT_CONFIG\n",
    "print(f\"PPO's default config is:\")\n",
    "pprint.pprint(PPO_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint.pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint.pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded886cc-436e-46cd-8fea-d68af8b41236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's do a very simple grid-search over two learning rates with tune.run().\n",
    "\n",
    "In particular, we will try the learning rates 0.00005 and 0.5 using `tune.grid_search([...])`\n",
    "inside our config dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063991e-173b-49be-a4e7-467e2e18321a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plugging in Ray Tune.\n",
    "# Note that this is the recommended way to run any experiments with RLlib.\n",
    "# Reasons:\n",
    "# - Tune allows you to do hyperparameter tuning in a user-friendly way\n",
    "#   and at large scale!\n",
    "# - Tune automatically allocates needed resources for the different\n",
    "#   hyperparam trials and experiment runs on a cluster.\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Running stuff with tune, we can re-use the exact\n",
    "# same config that we used when working with RLlib directly!\n",
    "tune_config = config.copy()\n",
    "\n",
    "# Let's add our first hyperparameter search via our config.\n",
    "# How about we try two different learning rates? Let's say 0.00005 and 0.5 (ouch!).\n",
    "tune_config[\"lr\"] = tune.grid_search([0.0001, 0.5])  # <- 0.5? again: ouch!\n",
    "tune_config[\"train_batch_size\"] = tune.grid_search([3000, 4000])\n",
    "\n",
    "# Now that we will run things \"automatically\" through tune, we have to\n",
    "# define one or more stopping criteria.\n",
    "# Tune will stop the run, once any single one of the criteria is matched (not all of them!).\n",
    "stop = {\n",
    "    # Note that the keys used here can be anything present in the above `rllib_trainer.train()` output dict.\n",
    "    \"training_iteration\": 5,\n",
    "    \"episode_reward_mean\": 20.0,\n",
    "}\n",
    "\n",
    "# \"PPO\" is a registered name that points to RLlib's PPOTrainer.\n",
    "# See `ray/rllib/agents/registry.py`\n",
    "\n",
    "# Run a simple experiment until one of the stopping criteria is met.\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "\n",
    "    # Note that no trainers will be returned from this call here.\n",
    "    # Tune will create n Trainers internally, run them in parallel and destroy them at the end.\n",
    "    # However, you can ...\n",
    "    checkpoint_at_end=True,  # ... create a checkpoint when done.\n",
    "    checkpoint_freq=10,  # ... create a checkpoint every 10 training iterations.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b886fb8-6ccd-4be2-80bb-fc0936808d11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why did we use 6 CPUs in the tune run above (3 CPUs per trial)?\n",
    "\n",
    "PPO - by default - uses 2 \"rollout\" workers (`num_workers=2`). These are Ray Actors that have their own environment copy(ies) and step through those in parallel. On top of these two \"rollout\" workers, every Trainer in RLlib always also has a \"local\" worker, which - in case of PPO - handles the learning updates. This gives us 3 workers (2 rollout + 1 local learner), which require 3 CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a74ec7-a6c1-431d-83aa-35df56d93185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise No 2\n",
    "\n",
    "<hr />\n",
    "\n",
    "Using the `tune_config` that we have built so far, let's run another `tune.run()`, but apply the following changes to our setup this time:\n",
    "- Setup only 1 learning rate under the \"lr\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Setup only 1 train batch size under the \"train_batch_size\" config key. Chose the (seemingly) best value from the run in the previous cell (the one that yielded the highest avg. reward).\n",
    "- Set `num_workers` to 5, which will allow us to run more environment \"rollouts\" in parallel and to collect training batches more quickly.\n",
    "- Set the `num_envs_per_worker` config parameter to 5. This will clone our env on each rollout worker, and thus parallelize action computing forward passes through our neural networks.\n",
    "\n",
    "Other than that, use the exact same args as in our `tune.run()` call in the previous cell.\n",
    "\n",
    "**Good luck! :)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff184330-4229-4476-a9e0-1fdbaed948d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-09-30 11:39:13 (running for 00:01:18.10)<br>Memory usage on this node: 33.5/125.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/61.35 GiB heap, 0.0/30.28 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/ajit/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 11:39:14,190\tINFO tune.py:747 -- Total run time: 78.38 seconds (78.05 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# !LIVE CODING!\n",
    "\n",
    "# Solution to Exercise #2\n",
    "\n",
    "# # Run for longer this time (100 iterations) and try to reach 40.0 reward (sum of both agents).\n",
    "stop = {\n",
    "    \"training_iteration\": 10,  # we have the 15min break now to run this many iterations\n",
    "    \"episode_reward_mean\": 60.0,  # sum of both agents' rewards. Probably won't reach it, but we should try nevertheless :)\n",
    "}\n",
    "\n",
    "# tune_config.update({\n",
    "# ???\n",
    "# })\n",
    "\n",
    "# analysis = tune.run(...)\n",
    "\n",
    "tune_config[\"lr\"] = 0.0001\n",
    "tune_config[\"train_batch_size\"] = 3000\n",
    "\n",
    "tune_config[\"num_workers\"] = 5\n",
    "tune_config[\"num_envs_per_worker\"] = 5\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config=tune_config,\n",
    "    stop=stop,\n",
    "    checkpoint_freq=5,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d282a-4ad1-4d5f-9dec-00afb8154048",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "------------------\n",
    "\n",
    "\n",
    "(while the above experiment is running (and hopefully learning))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do we extract any checkpoint from a trial of a tune.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5160e4d0-8feb-411d-a457-dfc10d50e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7f00401dab20>\n",
      "Found best checkpoint for trial #2: /home/ajit/ray_results/PPO/PPO_MultiAgentArena_39d0e_00000_0_2022-09-30_11-37-55/checkpoint_000010/checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the exercise) returned an Analysis object, from which we can access any checkpoint\n",
    "# (given we set checkpoint_freq or checkpoint_at_end to reasonable values) like so:\n",
    "print(analysis)\n",
    "# Get all trials (we only have one).\n",
    "trials = analysis.trials\n",
    "# Assuming, the first trial was the best, we'd like to extract this trial's best checkpoint \"\":\n",
    "best_checkpoint = analysis.get_best_checkpoint(trial=trials[0], metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(f\"Found best checkpoint for trial #2: {best_checkpoint}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac525070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 11:39:40,221\tWARNING ppo.py:386 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=5 num_envs_per_worker=5 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 160.\n",
      "2022-09-30 11:39:41,688\tINFO trainable.py:588 -- Restored on 10.13.62.8 from checkpoint: /home/ajit/ray_results/PPO/PPO_MultiAgentArena_39d0e_00000_0_2022-09-30_11-37-55/checkpoint_000010/checkpoint-10\n",
      "2022-09-30 11:39:41,691\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 71.87987494468689, '_episodes_total': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPOTrainer"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Undo the grid-search config, which RLlib doesn't understand.\n",
    "rllib_config = tune_config.copy()\n",
    "rllib_config[\"lr\"] = 0.00005\n",
    "rllib_config[\"train_batch_size\"] = 4000\n",
    "\n",
    "# Restore a RLlib Trainer from the checkpoint.\n",
    "new_trainer = PPOTrainer(config=rllib_config)\n",
    "new_trainer.restore(best_checkpoint)\n",
    "new_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a2e104d-72f8-4b80-bf9a-2f8cbd25d9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa19203d6ca849f4b39d1746e0377e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        a1 = new_trainer.compute_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "        a2 = new_trainer.compute_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "        actions = {\"agent1\": a1, \"agent2\": a2}\n",
    "        obs, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "        out.clear_output(wait=True)\n",
    "        env.render()\n",
    "        plt.show()\n",
    "        time.sleep(0.07)\n",
    "\n",
    "        if dones[\"agent1\"] is True:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3f56e-c4e1-4503-9ce5-589e826d1e5a",
   "metadata": {},
   "source": [
    "## Let's talk about customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f940b-697c-4d1d-af28-1d174331dc3c",
   "metadata": {},
   "source": [
    "### Deep Dive: How do we customize RLlib's RL loop?\n",
    "\n",
    "RLlib offers a callbacks API that allows you to add custom behavior to\n",
    "all major events during the environment sampling- and learning process.\n",
    "\n",
    "**Our problem:** So far, we can only see standard stats, such as rewards, episode lengths, etc..\n",
    "This does not give us enough insights sometimes into important questions, such as: How many times\n",
    "have both agents collided? or How many times has agent1 discovered a new field?\n",
    "\n",
    "In the following cell, we will create custom callback \"hooks\" that will allow us to\n",
    "add these stats to the returned metrics dict, and which will therefore be displayed in tensorboard!\n",
    "\n",
    "For that we will override RLlib's DefaultCallbacks class and implement the\n",
    "`on_episode_start`, `on_episode_step`, and `on_episode_end` methods therein:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb9733-01bc-426b-ad57-7983fc7db8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the DefaultCallbacks with your own and implement any methods (hooks)\n",
    "# that you need.\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self,\n",
    "                         *,\n",
    "                         worker,\n",
    "                         base_env,\n",
    "                         policies,\n",
    "                         episode: MultiAgentEpisode,\n",
    "                         env_index,\n",
    "                         **kwargs):\n",
    "        # We will use the `MultiAgentEpisode` object being passed into\n",
    "        # all episode-related callbacks. It comes with a user_data property (dict),\n",
    "        # which we can write arbitrary data into.\n",
    "\n",
    "        # At the end of an episode, we'll transfer that data into the `hist_data`, and `custom_metrics`\n",
    "        # properties to make sure our custom data is displayed in TensorBoard.\n",
    "\n",
    "        # The episode is starting:\n",
    "        # Set per-episode object to capture, which states (observations)\n",
    "        # have been visited by agent1.\n",
    "        episode.user_data[\"new_fields_discovered\"] = 0\n",
    "        # Set per-episode agent2-blocks counter (how many times has agent2 blocked agent1?).\n",
    "        episode.user_data[\"num_collisions\"] = 0\n",
    "\n",
    "    def on_episode_step(self,\n",
    "                        *,\n",
    "                        worker,\n",
    "                        base_env,\n",
    "                        episode: MultiAgentEpisode,\n",
    "                        env_index,\n",
    "                        **kwargs):\n",
    "        # Get both rewards.\n",
    "        ag1_r = episode.prev_reward_for(\"agent1\")\n",
    "        ag2_r = episode.prev_reward_for(\"agent2\")\n",
    "\n",
    "        # Agent1 discovered a new field.\n",
    "        if ag1_r == 1.0:\n",
    "            episode.user_data[\"new_fields_discovered\"] += 1\n",
    "        # Collision.\n",
    "        elif ag2_r == 1.0:\n",
    "            episode.user_data[\"num_collisions\"] += 1\n",
    "\n",
    "    def on_episode_end(self,\n",
    "                       *,\n",
    "                       worker,\n",
    "                       base_env,\n",
    "                       policies,\n",
    "                       episode: MultiAgentEpisode,\n",
    "                       env_index,\n",
    "                       **kwargs):\n",
    "        # Episode is done:\n",
    "        # Write scalar values (sum over rewards) to `custom_metrics` and\n",
    "        # time-series data (rewards per time step) to `hist_data`.\n",
    "        # Both will be visible then in TensorBoard.\n",
    "        episode.custom_metrics[\"new_fields_discovered\"] = episode.user_data[\"new_fields_discovered\"]\n",
    "        episode.custom_metrics[\"num_collisions\"] = episode.user_data[\"num_collisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fe8eb-c52f-4a26-9067-96ad9fe160a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up our config to point to our new custom callbacks class:\n",
    "config = {\n",
    "    \"env\": MultiAgentArena,\n",
    "    \"callbacks\": MyCallbacks,  # by default, this would point to `rllib.agents.callbacks.DefaultCallbacks`, which does nothing.\n",
    "    \"num_workers\": 5,  # we know now: this speeds up things!\n",
    "}\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    # If you'd like to restore the tune run from an existing checkpoint file, you can do the following:\n",
    "    #restore=\"/Users/sven/ray_results/PPO/PPO_MultiAgentArena_fd451_00000_0_2021-05-25_15-13-26/checkpoint_000010/checkpoint-10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa6a24",
   "metadata": {},
   "source": [
    "### Let's check tensorboard for the new custom metrics!\n",
    "\n",
    "1. Head over to the Anyscale project view and click on the \"TensorBoard\" butten:\n",
    "\n",
    "<img src=\"images/tensorboard_button.png\" width=1000>\n",
    "\n",
    "Alternatively - if you ran this locally on your own machine:\n",
    "\n",
    "1. Head over to ~/ray_results/PPO/PPO_MultiAgentArena_[some key]_00000_0_[date]_[time]/\n",
    "1. In that directory, you should see a `event.out....` file.\n",
    "1. Run `tensorboard --logdir .` and head to https://localhost:6006\n",
    "\n",
    "<img src=\"images/tensorboard.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac90dc-097d-4f10-b5ea-c4c1167f1f3a",
   "metadata": {},
   "source": [
    "### Deep Dive: Writing custom Models in tf or torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "\n",
    "tf1, tf, tf_version = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "# Custom Neural Network Models.\n",
    "class MyKerasModel(TFModelV2):\n",
    "    \"\"\"Custom model for policy gradient algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        super(MyKerasModel, self).__init__(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "        \n",
    "        # Keras Input layer.\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            name=\"layer1\",\n",
    "            activation=tf.nn.relu)(self.inputs)\n",
    "        \n",
    "        # Action logits output.\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"out\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        value_out = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            name=\"value\",\n",
    "            activation=None)(layer_1)\n",
    "\n",
    "        # The actual Keras model:\n",
    "        self.base_model = tf.keras.Model(self.inputs,\n",
    "                                         [logits, value_out])\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers and calculate the \"value\"\n",
    "        # of the observation and store it for when `value_function` is called.\n",
    "        logits, self.cur_value = self.base_model(input_dict[\"obs\"])\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return tf.reshape(self.cur_value, [-1])\n",
    "\n",
    "\n",
    "class MyTorchModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        \"\"\"Build a simple [16, 16]-MLP (+ value branch).\"\"\"\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hidden layer (shared by action logits outputs and value output).\n",
    "        self.layer_1 = nn.Linear(obs_space.shape[0], 16).to(self.device)\n",
    "\n",
    "        # Action logits output.\n",
    "        self.layer_out = nn.Linear(16, num_outputs).to(self.device)\n",
    "\n",
    "        # \"Value\"-branch (single node output).\n",
    "        # Used by several RLlib algorithms (e.g. PPO) to calculate an observation's value.\n",
    "        self.value_branch = nn.Linear(16, 1).to(self.device)\n",
    "        self.cur_value = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \"\"\"Custom-define your forard pass logic here.\"\"\"\n",
    "        # Pass inputs through our 2 layers.\n",
    "        layer_1_out = self.layer_1(input_dict[\"obs\"])\n",
    "        logits = self.layer_out(layer_1_out)\n",
    "\n",
    "        # Calculate the \"value\" of the observation and store it for\n",
    "        # when `value_function` is called.\n",
    "        self.cur_value = self.value_branch(layer_1_out).squeeze(1)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        \"\"\"Implement the value branch forward pass logic here:\n",
    "        \n",
    "        We will just return the already calculated `self.cur_value`.\n",
    "        \"\"\"\n",
    "        assert self.cur_value is not None, \"Must call `forward()` first!\"\n",
    "        return self.cur_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick test on the custom model classes.\n",
    "test_model_tf = MyKerasModel(\n",
    "    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "    action_space=None,\n",
    "    num_outputs=2,\n",
    "    model_config={},\n",
    "    name=\"MyModel\",\n",
    ")\n",
    "\n",
    "print(\"TF-output={}\".format(test_model_tf({\"obs\": np.array([[0.5, 0.5]])})))\n",
    "\n",
    "# For PyTorch, you can do:\n",
    "#test_model_torch = MyTorchModel(\n",
    "#    obs_space=gym.spaces.Box(-1.0, 1.0, (2, )),\n",
    "#    action_space=None,\n",
    "#    num_outputs=2,\n",
    "#    model_config={},\n",
    "#    name=\"MyModel\",\n",
    "#)\n",
    "#print(\"Torch-output={}\".format(test_model_torch({\"obs\": torch.from_numpy(np.array([[0.5, 0.5]], dtype=np.float32))})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our custom model and re-run the experiment.\n",
    "config.update({\n",
    "    \"model\": {\n",
    "        \"custom_model\": MyKerasModel,  # for torch users: \"custom_model\": MyTorchModel\n",
    "        \"custom_model_config\": {\n",
    "            #\"layers\": [128, 128],\n",
    "        },\n",
    "    },\n",
    "})\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,  # for torch users: config=dict(config, **{\"framework\": \"torch\"}),\n",
    "    stop={\n",
    "        \"training_iteration\": 5,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ad95",
   "metadata": {},
   "source": [
    "### Deep Dive: A closer look at RLlib's components\n",
    "#### (Depending on time left and amount of questions having been accumulated :)\n",
    "\n",
    "We already took a quick look inside an RLlib Trainer object and extracted its Policy(ies) and the Policy's model (neural network). Here is a much more detailed overview of what's inside a Trainer object.\n",
    "\n",
    "At the core is the so-called `WorkerSet` sitting under `Trainer.workers`. A WorkerSet is a group of `RolloutWorker` (`rllib.evaluation.rollout_worker.py`) objects that always consists of a \"local worker\" (`Trainer.workers.local_worker()`) and n \"remote workers\" (`Trainer.workers.remote_workers()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37549",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_structure.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d72883",
   "metadata": {},
   "source": [
    "### Scaling RLlib\n",
    "\n",
    "Scaling RLlib works by parallelizing the \"jobs\" that the remote `RolloutWorkers` do. In a vanilla RL algorithm, like PPO, DQN, and many others, the `@ray.remote` labeled RolloutWorkers in the figure above are responsible for interacting with one or more environments and thereby collecting experiences. Observations are produced by the environment, actions are then computed by the Policy(ies) copy located on the remote worker and sent to the environment in order to produce yet another observation. This cycle is repeated endlessly and only sometimes interrupted to send experience batches (\"train batches\") of a certain size to the \"local worker\". There these batches are used to call `Policy.learn_on_batch()`, which performs a loss calculation, followed by a model weights update, and a subsequent weights broadcast back to all the remote workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials.git\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/master/rllib.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f116b3-0962-44ff-9c08-558c6890abd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rllib-stable')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1b82eb6b91defcccb02b513f55e4a8555f19c3e8feea22de4e45418d1f17431"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
