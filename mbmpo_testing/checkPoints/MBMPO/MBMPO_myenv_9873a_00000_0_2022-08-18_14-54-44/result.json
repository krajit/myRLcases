{"episode_reward_max": 104.0, "episode_reward_min": 9.0, "episode_reward_mean": 25.503184713375795, "episode_len_mean": 25.503184713375795, "episode_media": {}, "episodes_this_iter": 157, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [18.0, 20.0, 20.0, 32.0, 20.0, 21.0, 15.0, 42.0, 38.0, 15.0, 45.0, 12.0, 29.0, 12.0, 14.0, 11.0, 24.0, 12.0, 14.0, 15.0, 38.0, 10.0, 41.0, 15.0, 15.0, 26.0, 25.0, 26.0, 18.0, 17.0, 48.0, 17.0, 20.0, 44.0, 25.0, 26.0, 13.0, 16.0, 43.0, 15.0, 34.0, 11.0, 35.0, 23.0, 56.0, 25.0, 17.0, 16.0, 32.0, 37.0, 19.0, 15.0, 35.0, 57.0, 37.0, 24.0, 25.0, 42.0, 37.0, 47.0, 20.0, 14.0, 32.0, 40.0, 16.0, 14.0, 14.0, 12.0, 36.0, 20.0, 20.0, 12.0, 24.0, 25.0, 29.0, 10.0, 52.0, 14.0, 15.0, 17.0, 15.0, 30.0, 19.0, 73.0, 14.0, 20.0, 17.0, 13.0, 17.0, 17.0, 31.0, 18.0, 15.0, 30.0, 38.0, 15.0, 36.0, 46.0, 38.0, 16.0, 12.0, 14.0, 22.0, 11.0, 39.0, 11.0, 18.0, 22.0, 16.0, 67.0, 11.0, 31.0, 14.0, 87.0, 45.0, 9.0, 12.0, 19.0, 15.0, 50.0, 43.0, 14.0, 25.0, 25.0, 28.0, 23.0, 17.0, 104.0, 32.0, 22.0, 18.0, 21.0, 32.0, 20.0, 100.0, 21.0, 13.0, 14.0, 43.0, 19.0, 25.0, 30.0, 12.0, 17.0, 27.0, 12.0, 16.0, 15.0, 22.0, 16.0, 19.0, 33.0, 15.0, 16.0, 15.0, 30.0, 20.0], "episode_lengths": [18, 20, 20, 32, 20, 21, 15, 42, 38, 15, 45, 12, 29, 12, 14, 11, 24, 12, 14, 15, 38, 10, 41, 15, 15, 26, 25, 26, 18, 17, 48, 17, 20, 44, 25, 26, 13, 16, 43, 15, 34, 11, 35, 23, 56, 25, 17, 16, 32, 37, 19, 15, 35, 57, 37, 24, 25, 42, 37, 47, 20, 14, 32, 40, 16, 14, 14, 12, 36, 20, 20, 12, 24, 25, 29, 10, 52, 14, 15, 17, 15, 30, 19, 73, 14, 20, 17, 13, 17, 17, 31, 18, 15, 30, 38, 15, 36, 46, 38, 16, 12, 14, 22, 11, 39, 11, 18, 22, 16, 67, 11, 31, 14, 87, 45, 9, 12, 19, 15, 50, 43, 14, 25, 25, 28, 23, 17, 104, 32, 22, 18, 21, 32, 20, 100, 21, 13, 14, 43, 19, 25, 30, 12, 17, 27, 12, 16, 15, 22, 16, 19, 33, 15, 16, 15, 30, 20]}, "sampler_perf": {"mean_raw_obs_processing_ms": 9.417046671328336, "mean_inference_ms": 2.39738865175109, "mean_action_processing_ms": 0.6912349694784906, "mean_env_wait_ms": 0.6792579872020778, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 8038, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [1.5625e-12, 3.90625e-13], "cur_lr": 0.001, "total_loss": 23.455142974853516, "policy_loss": -0.002296180697157979, "vf_loss": 46.90569305419922, "kl_loss": 0.002541785128414631, "inner_kl": [0.002272950718179345, 0.002787965815514326], "entropy": 0.5628118515014648}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 240000, "num_steps_sampled": 8038}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 20.375, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 10.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 53.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 25.35, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 10.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 75.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 20.875, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 50.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 21.475, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 8.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 46.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 28.125, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 71.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 21.6, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 10.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 48.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 30.425, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 77.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 25.0, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 80.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 30.55, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 8.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 77.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 25.25, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 58.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 25.425, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 8.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 68.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 30.025, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 9.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 85.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 26.125, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 10.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 56.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 25.05, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 8.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 65.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 28.475, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 70.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 28.325, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 7.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 138.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 28.4, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 11.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 96.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 29.325, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 9.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 81.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 32.25, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 8.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 122.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 32.475, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 10.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 76.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 28.075, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 64.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 34.3, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 8.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 94.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 26.65, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 11.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 73.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 29.775, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 9.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 78.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 32.4, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 11.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 61.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 35.05, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 15.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 104.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 36.3, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 11.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 104.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 32.05, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 10.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 77.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 31.45, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 107.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 30.425, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 10.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 83.0, "num_recreated_workers": 0, "done": false, "episodes_total": 157, "training_iteration": 1, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-05-51", "timestamp": 1660815351, "time_this_iter_s": 265.1170492172241, "time_total_s": 265.1170492172241, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d23d8d90>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f092ce49c70>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 265.1170492172241, "timesteps_since_restore": 16000, "iterations_since_restore": 1, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 26.952785145888598, "ram_util_percent": 31.839522546419097}}
{"episode_reward_max": 108.0, "episode_reward_min": 10.0, "episode_reward_mean": 27.08783783783784, "episode_len_mean": 27.08783783783784, "episode_media": {}, "episodes_this_iter": 148, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [23.0, 33.0, 21.0, 37.0, 12.0, 16.0, 52.0, 14.0, 17.0, 43.0, 21.0, 32.0, 16.0, 19.0, 54.0, 48.0, 44.0, 53.0, 20.0, 77.0, 41.0, 36.0, 15.0, 29.0, 18.0, 32.0, 18.0, 32.0, 28.0, 77.0, 16.0, 27.0, 20.0, 26.0, 24.0, 12.0, 13.0, 48.0, 17.0, 24.0, 15.0, 20.0, 27.0, 50.0, 58.0, 19.0, 22.0, 35.0, 12.0, 13.0, 11.0, 33.0, 24.0, 24.0, 15.0, 39.0, 20.0, 27.0, 22.0, 16.0, 25.0, 18.0, 40.0, 58.0, 25.0, 28.0, 22.0, 53.0, 20.0, 22.0, 23.0, 21.0, 18.0, 108.0, 20.0, 21.0, 20.0, 27.0, 15.0, 15.0, 13.0, 14.0, 19.0, 28.0, 23.0, 24.0, 14.0, 46.0, 40.0, 12.0, 17.0, 10.0, 19.0, 14.0, 12.0, 14.0, 29.0, 37.0, 43.0, 19.0, 27.0, 13.0, 23.0, 18.0, 46.0, 11.0, 21.0, 11.0, 11.0, 14.0, 10.0, 12.0, 28.0, 51.0, 16.0, 17.0, 21.0, 31.0, 48.0, 20.0, 21.0, 16.0, 16.0, 30.0, 23.0, 16.0, 16.0, 18.0, 73.0, 53.0, 11.0, 14.0, 29.0, 32.0, 45.0, 12.0, 19.0, 23.0, 27.0, 25.0, 19.0, 54.0, 54.0, 12.0, 13.0, 47.0, 63.0, 26.0], "episode_lengths": [23, 33, 21, 37, 12, 16, 52, 14, 17, 43, 21, 32, 16, 19, 54, 48, 44, 53, 20, 77, 41, 36, 15, 29, 18, 32, 18, 32, 28, 77, 16, 27, 20, 26, 24, 12, 13, 48, 17, 24, 15, 20, 27, 50, 58, 19, 22, 35, 12, 13, 11, 33, 24, 24, 15, 39, 20, 27, 22, 16, 25, 18, 40, 58, 25, 28, 22, 53, 20, 22, 23, 21, 18, 108, 20, 21, 20, 27, 15, 15, 13, 14, 19, 28, 23, 24, 14, 46, 40, 12, 17, 10, 19, 14, 12, 14, 29, 37, 43, 19, 27, 13, 23, 18, 46, 11, 21, 11, 11, 14, 10, 12, 28, 51, 16, 17, 21, 31, 48, 20, 21, 16, 16, 30, 23, 16, 16, 18, 73, 53, 11, 14, 29, 32, 45, 12, 19, 23, 27, 25, 19, 54, 54, 12, 13, 47, 63, 26]}, "sampler_perf": {"mean_raw_obs_processing_ms": 9.961033099469189, "mean_inference_ms": 2.6498418074744747, "mean_action_processing_ms": 0.7482349437518712, "mean_env_wait_ms": 0.7279560223693107, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 12047, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [4.8828125e-14, 6.103515625e-15], "cur_lr": 0.001, "total_loss": 45.03267288208008, "policy_loss": 0.002954621333628893, "vf_loss": 90.07125854492188, "kl_loss": 0.00701605249196291, "inner_kl": [0.007029988337308168, 0.006991502363234758], "entropy": 0.36196401715278625}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 480000, "num_steps_sampled": 12047}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 32.525, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 14.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 97.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 33.5, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 17.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 88.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 29.675, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 10.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 114.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 32.175, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 70.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 37.775, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 9.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 82.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 34.0, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 83.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 36.4, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 14.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 138.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 43.2, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 14.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 171.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 32.625, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 12.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 59.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 38.075, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 137.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 36.55, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 15.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 105.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 41.3, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 16.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 92.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 39.2, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 10.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 102.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 40.675, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 17.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 131.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 34.575, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 10.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 63.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 35.9, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 14.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 99.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 34.05, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 10.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 76.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 36.7, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 78.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 37.875, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 18.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 115.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 34.775, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 11.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 82.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 39.5, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 20.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 107.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 34.725, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 18.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 81.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 45.475, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 15.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 87.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 40.45, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 20.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 98.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 32.85, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 12.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 83.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 31.7, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 9.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 55.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 37.575, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 20.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 113.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 40.4, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 17.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 109.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 49.6, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 19.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 107.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 43.9, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 21.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 96.0, "num_recreated_workers": 0, "done": false, "episodes_total": 305, "training_iteration": 2, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-11-24", "timestamp": 1660815684, "time_this_iter_s": 332.3432493209839, "time_total_s": 597.460298538208, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d234b4c0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d234b760>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 597.460298538208, "timesteps_since_restore": 32000, "iterations_since_restore": 2, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 36.145762711864414, "ram_util_percent": 35.21673728813559}}
{"episode_reward_max": 116.0, "episode_reward_min": 9.0, "episode_reward_mean": 27.666666666666668, "episode_len_mean": 27.666666666666668, "episode_media": {}, "episodes_this_iter": 147, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [16.0, 27.0, 39.0, 31.0, 11.0, 16.0, 29.0, 37.0, 38.0, 18.0, 92.0, 18.0, 24.0, 27.0, 11.0, 14.0, 18.0, 16.0, 49.0, 37.0, 25.0, 40.0, 64.0, 35.0, 18.0, 12.0, 15.0, 15.0, 28.0, 14.0, 25.0, 21.0, 14.0, 26.0, 30.0, 32.0, 13.0, 14.0, 18.0, 29.0, 28.0, 20.0, 11.0, 23.0, 20.0, 27.0, 46.0, 14.0, 16.0, 31.0, 23.0, 104.0, 39.0, 17.0, 15.0, 29.0, 14.0, 38.0, 66.0, 16.0, 21.0, 26.0, 56.0, 64.0, 45.0, 34.0, 22.0, 39.0, 14.0, 20.0, 32.0, 24.0, 15.0, 23.0, 15.0, 19.0, 27.0, 17.0, 18.0, 15.0, 13.0, 19.0, 14.0, 28.0, 15.0, 16.0, 13.0, 41.0, 22.0, 27.0, 43.0, 22.0, 18.0, 19.0, 12.0, 30.0, 51.0, 28.0, 16.0, 32.0, 26.0, 36.0, 22.0, 35.0, 14.0, 31.0, 13.0, 21.0, 24.0, 94.0, 25.0, 15.0, 23.0, 50.0, 28.0, 12.0, 22.0, 59.0, 16.0, 59.0, 24.0, 12.0, 13.0, 20.0, 33.0, 9.0, 59.0, 18.0, 33.0, 29.0, 26.0, 15.0, 17.0, 17.0, 25.0, 20.0, 57.0, 14.0, 16.0, 26.0, 22.0, 21.0, 14.0, 75.0, 17.0, 16.0, 116.0], "episode_lengths": [16, 27, 39, 31, 11, 16, 29, 37, 38, 18, 92, 18, 24, 27, 11, 14, 18, 16, 49, 37, 25, 40, 64, 35, 18, 12, 15, 15, 28, 14, 25, 21, 14, 26, 30, 32, 13, 14, 18, 29, 28, 20, 11, 23, 20, 27, 46, 14, 16, 31, 23, 104, 39, 17, 15, 29, 14, 38, 66, 16, 21, 26, 56, 64, 45, 34, 22, 39, 14, 20, 32, 24, 15, 23, 15, 19, 27, 17, 18, 15, 13, 19, 14, 28, 15, 16, 13, 41, 22, 27, 43, 22, 18, 19, 12, 30, 51, 28, 16, 32, 26, 36, 22, 35, 14, 31, 13, 21, 24, 94, 25, 15, 23, 50, 28, 12, 22, 59, 16, 59, 24, 12, 13, 20, 33, 9, 59, 18, 33, 29, 26, 15, 17, 17, 25, 20, 57, 14, 16, 26, 22, 21, 14, 75, 17, 16, 116]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.446165355981563, "mean_inference_ms": 2.855126530516381, "mean_action_processing_ms": 0.780367617513619, "mean_env_wait_ms": 0.7771634588054583, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 16114, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [1.953125e-13, 9.765625e-14], "cur_lr": 0.001, "total_loss": 49.289310455322266, "policy_loss": 0.0034305944573134184, "vf_loss": 98.58547973632812, "kl_loss": 0.0055077215656638145, "inner_kl": [0.006018113344907761, 0.005191807635128498], "entropy": 0.5451325178146362}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 720000, "num_steps_sampled": 16114}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 43.525, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 13.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 144.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 35.925, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 13.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 99.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 52.3, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 17.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 111.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 48.1, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 15.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 122.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 42.125, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 20.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 110.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 48.65, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 18.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 121.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 47.775, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 22.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 103.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 47.225, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 16.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 104.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 59.9, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 24.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 134.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 55.725, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 23.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 113.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 45.1, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 23.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 94.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 46.0, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 20.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 105.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 49.175, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 13.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 165.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 55.275, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 21.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 125.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 53.325, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 24.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 114.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 56.625, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 15.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 140.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 50.35, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 22.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 130.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 62.5, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 20.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 156.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 54.6, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 16.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 118.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 55.075, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 19.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 116.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 55.575, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 22.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 119.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 51.9, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 19.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 163.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 59.85, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 21.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 60.425, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 26.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 45.85, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 15.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 90.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 53.55, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 18.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 141.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 64.4, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 28.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 165.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 55.6, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 23.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 141.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 68.275, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 30.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 129.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 63.025, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 31.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 108.0, "num_recreated_workers": 0, "done": false, "episodes_total": 452, "training_iteration": 3, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-17-21", "timestamp": 1660816041, "time_this_iter_s": 357.45589542388916, "time_total_s": 954.9161939620972, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d241c850>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d241c040>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 954.9161939620972, "timesteps_since_restore": 48000, "iterations_since_restore": 3, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 39.59015748031496, "ram_util_percent": 40.606889763779535}}
{"episode_reward_max": 67.0, "episode_reward_min": 9.0, "episode_reward_mean": 24.75925925925926, "episode_len_mean": 24.75925925925926, "episode_media": {}, "episodes_this_iter": 162, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [22.0, 46.0, 23.0, 29.0, 15.0, 14.0, 21.0, 22.0, 15.0, 13.0, 17.0, 27.0, 15.0, 19.0, 16.0, 16.0, 24.0, 49.0, 21.0, 27.0, 56.0, 15.0, 30.0, 12.0, 35.0, 52.0, 33.0, 14.0, 29.0, 18.0, 18.0, 9.0, 18.0, 15.0, 14.0, 13.0, 14.0, 25.0, 19.0, 17.0, 17.0, 16.0, 13.0, 20.0, 21.0, 31.0, 36.0, 13.0, 16.0, 24.0, 38.0, 19.0, 27.0, 39.0, 11.0, 12.0, 18.0, 23.0, 17.0, 13.0, 46.0, 14.0, 14.0, 21.0, 18.0, 12.0, 31.0, 60.0, 15.0, 18.0, 13.0, 27.0, 38.0, 15.0, 28.0, 24.0, 62.0, 17.0, 56.0, 16.0, 61.0, 27.0, 16.0, 15.0, 25.0, 18.0, 39.0, 19.0, 22.0, 16.0, 23.0, 23.0, 25.0, 28.0, 16.0, 46.0, 25.0, 14.0, 15.0, 37.0, 37.0, 28.0, 12.0, 29.0, 67.0, 19.0, 31.0, 20.0, 20.0, 20.0, 21.0, 29.0, 14.0, 13.0, 23.0, 16.0, 31.0, 27.0, 27.0, 18.0, 11.0, 21.0, 14.0, 16.0, 20.0, 64.0, 21.0, 22.0, 22.0, 38.0, 15.0, 26.0, 14.0, 15.0, 25.0, 26.0, 56.0, 59.0, 43.0, 17.0, 14.0, 23.0, 25.0, 25.0, 20.0, 29.0, 42.0, 26.0, 18.0, 28.0, 15.0, 11.0, 58.0, 18.0, 14.0, 34.0, 38.0, 37.0, 39.0, 9.0, 19.0, 36.0], "episode_lengths": [22, 46, 23, 29, 15, 14, 21, 22, 15, 13, 17, 27, 15, 19, 16, 16, 24, 49, 21, 27, 56, 15, 30, 12, 35, 52, 33, 14, 29, 18, 18, 9, 18, 15, 14, 13, 14, 25, 19, 17, 17, 16, 13, 20, 21, 31, 36, 13, 16, 24, 38, 19, 27, 39, 11, 12, 18, 23, 17, 13, 46, 14, 14, 21, 18, 12, 31, 60, 15, 18, 13, 27, 38, 15, 28, 24, 62, 17, 56, 16, 61, 27, 16, 15, 25, 18, 39, 19, 22, 16, 23, 23, 25, 28, 16, 46, 25, 14, 15, 37, 37, 28, 12, 29, 67, 19, 31, 20, 20, 20, 21, 29, 14, 13, 23, 16, 31, 27, 27, 18, 11, 21, 14, 16, 20, 64, 21, 22, 22, 38, 15, 26, 14, 15, 25, 26, 56, 59, 43, 17, 14, 23, 25, 25, 20, 29, 42, 26, 18, 28, 15, 11, 58, 18, 14, 34, 38, 37, 39, 9, 19, 36]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.66979250222275, "mean_inference_ms": 2.993944502767753, "mean_action_processing_ms": 0.802733082775175, "mean_env_wait_ms": 0.8071371445706179, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 20125, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [2e-10, 2e-10], "cur_lr": 0.001, "total_loss": 62.356319427490234, "policy_loss": -0.0012620417401194572, "vf_loss": 124.71011352539062, "kl_loss": 0.004236562643200159, "inner_kl": [0.0032096083741635084, 0.0033269173000007868], "entropy": 0.5752602219581604}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 960000, "num_steps_sampled": 20125}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 63.6, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 32.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 126.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 57.925, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 28.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 101.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 69.125, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 26.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 156.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 66.55, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 39.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 125.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 60.55, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 39.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 101.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 62.425, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 25.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 153.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 49.875, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 18.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 113.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 56.075, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 17.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 125.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 66.0, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 21.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 187.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 59.975, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 24.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 160.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 67.275, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 22.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 143.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 65.125, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 34.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 120.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 48.025, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 24.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 91.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 51.525, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 22.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 118.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 79.175, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 51.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 149.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 72.4, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 36.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 139.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 64.0, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 28.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 150.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 58.55, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 28.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 126.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 67.075, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 29.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 162.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 72.625, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 34.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 158.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 80.475, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 27.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 167.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 71.225, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 33.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 179.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 61.275, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 32.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 102.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 61.575, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 25.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 149.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 70.225, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 29.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 144.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 64.15, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 25.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 137.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 61.45, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 22.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 127.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 68.075, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 26.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 125.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 82.775, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 35.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 166.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 70.15, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 29.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 179.0, "num_recreated_workers": 0, "done": false, "episodes_total": 614, "training_iteration": 4, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-23-57", "timestamp": 1660816437, "time_this_iter_s": 395.8252623081207, "time_total_s": 1350.741456270218, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2346ee0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2346e50>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 1350.741456270218, "timesteps_since_restore": 64000, "iterations_since_restore": 4, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 43.24028520499109, "ram_util_percent": 42.02317290552585}}
{"episode_reward_max": 107.0, "episode_reward_min": 11.0, "episode_reward_mean": 29.291970802919707, "episode_len_mean": 29.291970802919707, "episode_media": {}, "episodes_this_iter": 137, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [17.0, 18.0, 23.0, 34.0, 27.0, 41.0, 60.0, 22.0, 19.0, 27.0, 54.0, 12.0, 14.0, 66.0, 27.0, 22.0, 27.0, 23.0, 18.0, 44.0, 83.0, 19.0, 50.0, 21.0, 20.0, 33.0, 22.0, 32.0, 31.0, 25.0, 15.0, 20.0, 13.0, 38.0, 21.0, 74.0, 14.0, 29.0, 24.0, 15.0, 49.0, 16.0, 27.0, 16.0, 17.0, 14.0, 22.0, 20.0, 14.0, 19.0, 21.0, 12.0, 29.0, 28.0, 15.0, 42.0, 25.0, 21.0, 31.0, 30.0, 33.0, 11.0, 18.0, 21.0, 42.0, 78.0, 43.0, 28.0, 45.0, 20.0, 24.0, 19.0, 32.0, 12.0, 12.0, 20.0, 58.0, 27.0, 17.0, 65.0, 16.0, 36.0, 53.0, 33.0, 28.0, 27.0, 14.0, 28.0, 33.0, 49.0, 13.0, 22.0, 21.0, 15.0, 59.0, 19.0, 15.0, 36.0, 33.0, 14.0, 46.0, 21.0, 22.0, 16.0, 12.0, 60.0, 23.0, 16.0, 23.0, 17.0, 13.0, 35.0, 36.0, 37.0, 25.0, 11.0, 20.0, 25.0, 25.0, 32.0, 73.0, 27.0, 86.0, 15.0, 80.0, 23.0, 18.0, 32.0, 12.0, 17.0, 34.0, 44.0, 29.0, 16.0, 21.0, 13.0, 107.0], "episode_lengths": [17, 18, 23, 34, 27, 41, 60, 22, 19, 27, 54, 12, 14, 66, 27, 22, 27, 23, 18, 44, 83, 19, 50, 21, 20, 33, 22, 32, 31, 25, 15, 20, 13, 38, 21, 74, 14, 29, 24, 15, 49, 16, 27, 16, 17, 14, 22, 20, 14, 19, 21, 12, 29, 28, 15, 42, 25, 21, 31, 30, 33, 11, 18, 21, 42, 78, 43, 28, 45, 20, 24, 19, 32, 12, 12, 20, 58, 27, 17, 65, 16, 36, 53, 33, 28, 27, 14, 28, 33, 49, 13, 22, 21, 15, 59, 19, 15, 36, 33, 14, 46, 21, 22, 16, 12, 60, 23, 16, 23, 17, 13, 35, 36, 37, 25, 11, 20, 25, 25, 32, 73, 27, 86, 15, 80, 23, 18, 32, 12, 17, 34, 44, 29, 16, 21, 13, 107]}, "sampler_perf": {"mean_raw_obs_processing_ms": 11.000246164096326, "mean_inference_ms": 3.1610664744073484, "mean_action_processing_ms": 0.8370841275235837, "mean_env_wait_ms": 0.8405436161433676, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 24138, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [2e-10, 2e-10], "cur_lr": 0.001, "total_loss": 63.12590789794922, "policy_loss": -0.0016090628923848271, "vf_loss": 126.24858856201172, "kl_loss": 0.0015919686993584037, "inner_kl": [0.0010804389603435993, 0.001191260409541428], "entropy": 0.5461154580116272}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 1200000, "num_steps_sampled": 24138}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 51.45, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 26.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 148.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 65.025, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 28.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 136.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 75.475, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 31.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 189.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 72.0, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 24.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 172.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 69.525, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 33.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 119.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 64.3, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 31.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 118.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 63.025, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 29.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 128.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 73.45, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 32.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 141.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 75.55, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 33.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 168.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 80.85, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 34.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 177.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 80.775, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 30.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 181.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 70.275, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 36.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 160.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 71.35, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 31.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 65.4, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 25.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 153.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 60.0, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 32.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 149.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 60.3, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 28.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 123.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 88.125, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 36.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 76.275, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 37.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 185.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 69.3, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 36.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 161.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 68.025, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 30.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 166.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 70.3, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 32.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 135.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 74.85, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 28.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 130.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 70.025, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 28.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 183.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 73.625, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 30.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 142.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 93.6, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 50.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 188.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 72.55, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 33.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 134.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 63.575, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 24.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 143.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 73.425, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 29.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 139.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 83.325, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 31.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 187.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 77.65, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 29.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 200.0, "num_recreated_workers": 0, "done": false, "episodes_total": 751, "training_iteration": 5, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-30-27", "timestamp": 1660816827, "time_this_iter_s": 389.7066328525543, "time_total_s": 1740.4480891227722, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f092cb6c250>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d233ad00>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 1740.4480891227722, "timesteps_since_restore": 80000, "iterations_since_restore": 5, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 36.41193490054249, "ram_util_percent": 42.67197106690777}}
{"episode_reward_max": 184.0, "episode_reward_min": 10.0, "episode_reward_mean": 31.59375, "episode_len_mean": 31.59375, "episode_media": {}, "episodes_this_iter": 128, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [15.0, 52.0, 17.0, 13.0, 16.0, 13.0, 33.0, 13.0, 17.0, 18.0, 12.0, 30.0, 23.0, 35.0, 30.0, 24.0, 41.0, 12.0, 157.0, 17.0, 34.0, 59.0, 25.0, 15.0, 10.0, 34.0, 20.0, 42.0, 40.0, 20.0, 27.0, 14.0, 32.0, 17.0, 32.0, 21.0, 13.0, 66.0, 129.0, 19.0, 24.0, 35.0, 14.0, 18.0, 24.0, 10.0, 18.0, 21.0, 28.0, 48.0, 13.0, 21.0, 51.0, 12.0, 19.0, 28.0, 18.0, 45.0, 24.0, 12.0, 60.0, 58.0, 38.0, 44.0, 17.0, 25.0, 21.0, 20.0, 34.0, 20.0, 21.0, 19.0, 48.0, 53.0, 45.0, 17.0, 40.0, 38.0, 24.0, 17.0, 19.0, 19.0, 25.0, 68.0, 32.0, 19.0, 11.0, 29.0, 59.0, 26.0, 24.0, 41.0, 32.0, 16.0, 125.0, 23.0, 16.0, 20.0, 15.0, 47.0, 21.0, 38.0, 19.0, 21.0, 16.0, 20.0, 75.0, 15.0, 14.0, 146.0, 23.0, 18.0, 17.0, 11.0, 20.0, 21.0, 15.0, 43.0, 184.0, 28.0, 42.0, 18.0, 13.0, 14.0, 14.0, 20.0, 20.0, 73.0], "episode_lengths": [15, 52, 17, 13, 16, 13, 33, 13, 17, 18, 12, 30, 23, 35, 30, 24, 41, 12, 157, 17, 34, 59, 25, 15, 10, 34, 20, 42, 40, 20, 27, 14, 32, 17, 32, 21, 13, 66, 129, 19, 24, 35, 14, 18, 24, 10, 18, 21, 28, 48, 13, 21, 51, 12, 19, 28, 18, 45, 24, 12, 60, 58, 38, 44, 17, 25, 21, 20, 34, 20, 21, 19, 48, 53, 45, 17, 40, 38, 24, 17, 19, 19, 25, 68, 32, 19, 11, 29, 59, 26, 24, 41, 32, 16, 125, 23, 16, 20, 15, 47, 21, 38, 19, 21, 16, 20, 75, 15, 14, 146, 23, 18, 17, 11, 20, 21, 15, 43, 184, 28, 42, 18, 13, 14, 14, 20, 20, 73]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.891872871699226, "mean_inference_ms": 3.0852481447902558, "mean_action_processing_ms": 0.9614015124087554, "mean_env_wait_ms": 0.8256372343863364, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 28182, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [1.6e-09, 3.2e-09], "cur_lr": 0.001, "total_loss": 90.35417175292969, "policy_loss": -0.0008287837845273316, "vf_loss": 180.70669555664062, "kl_loss": 0.005406884476542473, "inner_kl": [0.0018728951690718532, 0.0022457146551460028], "entropy": 0.5685899257659912}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 1440000, "num_steps_sampled": 28182}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 96.5, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 50.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 160.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 79.5, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 23.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 166.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 82.0, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 48.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 142.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 77.7, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 39.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 172.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 84.275, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 46.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 192.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 87.125, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 56.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 151.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 104.725, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 63.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 194.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 92.9, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 52.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 159.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 91.3, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 42.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 157.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 82.275, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 39.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 164.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 74.15, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 39.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 145.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 92.875, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 28.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 190.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 107.55, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 44.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 169.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 91.275, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 48.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 175.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 84.05, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 36.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 149.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 99.725, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 58.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 144.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 92.875, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 57.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 175.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 87.85, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 54.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 165.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 100.525, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 45.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 185.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 77.425, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 42.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 130.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 75.375, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 32.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 128.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 87.275, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 31.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 142.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 87.075, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 40.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 178.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 94.425, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 43.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 163.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 97.275, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 33.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 164.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 90.95, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 30.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 85.225, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 52.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 136.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 94.925, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 44.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 189.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 102.1, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 41.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 187.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 96.525, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 44.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 185.0, "num_recreated_workers": 0, "done": false, "episodes_total": 879, "training_iteration": 6, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-34-37", "timestamp": 1660817077, "time_this_iter_s": 249.97600173950195, "time_total_s": 1990.4240908622742, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2395070>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2395340>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 1990.4240908622742, "timesteps_since_restore": 96000, "iterations_since_restore": 6, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 40.583661971830985, "ram_util_percent": 42.17830985915493}}
{"episode_reward_max": 200.0, "episode_reward_min": 10.0, "episode_reward_mean": 28.47517730496454, "episode_len_mean": 28.47517730496454, "episode_media": {}, "episodes_this_iter": 141, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [23.0, 13.0, 19.0, 15.0, 20.0, 12.0, 31.0, 16.0, 19.0, 11.0, 12.0, 19.0, 13.0, 31.0, 27.0, 18.0, 24.0, 29.0, 54.0, 36.0, 20.0, 35.0, 34.0, 18.0, 16.0, 22.0, 17.0, 18.0, 21.0, 20.0, 18.0, 37.0, 15.0, 24.0, 21.0, 54.0, 41.0, 20.0, 27.0, 25.0, 18.0, 50.0, 20.0, 29.0, 200.0, 22.0, 75.0, 16.0, 32.0, 49.0, 11.0, 12.0, 20.0, 27.0, 25.0, 17.0, 11.0, 13.0, 19.0, 44.0, 30.0, 16.0, 15.0, 34.0, 18.0, 102.0, 12.0, 33.0, 18.0, 22.0, 13.0, 36.0, 24.0, 17.0, 55.0, 18.0, 15.0, 32.0, 24.0, 41.0, 24.0, 12.0, 15.0, 19.0, 12.0, 12.0, 17.0, 48.0, 12.0, 21.0, 117.0, 48.0, 26.0, 81.0, 17.0, 28.0, 22.0, 21.0, 15.0, 16.0, 17.0, 27.0, 26.0, 17.0, 31.0, 14.0, 44.0, 22.0, 10.0, 15.0, 15.0, 51.0, 35.0, 15.0, 14.0, 18.0, 40.0, 19.0, 16.0, 37.0, 30.0, 26.0, 22.0, 22.0, 21.0, 21.0, 200.0, 45.0, 24.0, 23.0, 17.0, 24.0, 26.0, 41.0, 22.0, 23.0, 24.0, 16.0, 11.0, 15.0, 66.0], "episode_lengths": [23, 13, 19, 15, 20, 12, 31, 16, 19, 11, 12, 19, 13, 31, 27, 18, 24, 29, 54, 36, 20, 35, 34, 18, 16, 22, 17, 18, 21, 20, 18, 37, 15, 24, 21, 54, 41, 20, 27, 25, 18, 50, 20, 29, 200, 22, 75, 16, 32, 49, 11, 12, 20, 27, 25, 17, 11, 13, 19, 44, 30, 16, 15, 34, 18, 102, 12, 33, 18, 22, 13, 36, 24, 17, 55, 18, 15, 32, 24, 41, 24, 12, 15, 19, 12, 12, 17, 48, 12, 21, 117, 48, 26, 81, 17, 28, 22, 21, 15, 16, 17, 27, 26, 17, 31, 14, 44, 22, 10, 15, 15, 51, 35, 15, 14, 18, 40, 19, 16, 37, 30, 26, 22, 22, 21, 21, 200, 45, 24, 23, 17, 24, 26, 41, 22, 23, 24, 16, 11, 15, 66]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.702975972771126, "mean_inference_ms": 3.010128653205639, "mean_action_processing_ms": 0.92989141243514, "mean_env_wait_ms": 0.8107823315666233, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 32197, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [5e-11, 4e-10], "cur_lr": 0.001, "total_loss": 84.06194305419922, "policy_loss": 0.002532939426600933, "vf_loss": 168.1289520263672, "kl_loss": 0.002629453781992197, "inner_kl": [0.0015724706463515759, 0.0014096831437200308], "entropy": 0.5348106622695923}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 1680000, "num_steps_sampled": 32197}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 87.875, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 43.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 175.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 92.825, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 42.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 189.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 83.575, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 42.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 174.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 93.15, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 45.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 176.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 121.25, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 53.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 192.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 84.15, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 29.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 162.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 94.725, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 42.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 148.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 101.375, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 58.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 189.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 81.625, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 40.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 157.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 88.725, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 55.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 171.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 93.65, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 40.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 166.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 100.35, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 48.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 167.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 115.975, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 47.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 98.225, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 43.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 185.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 97.95, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 43.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 187.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 91.15, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 40.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 168.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 94.4, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 51.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 166.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 95.175, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 45.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 158.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 91.8, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 40.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 177.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 95.05, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 36.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 193.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 89.025, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 41.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 139.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 102.275, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 48.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 159.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 107.825, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 65.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 188.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 101.175, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 41.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 102.675, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 46.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 180.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 109.975, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 47.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 186.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 97.075, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 54.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 162.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 94.75, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 39.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 176.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 102.175, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 54.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 180.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 96.875, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 53.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 143.0, "num_recreated_workers": 0, "done": false, "episodes_total": 1020, "training_iteration": 7, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-38-20", "timestamp": 1660817300, "time_this_iter_s": 222.88669848442078, "time_total_s": 2213.310789346695, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2345100>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2345fd0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 2213.310789346695, "timesteps_since_restore": 112000, "iterations_since_restore": 7, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 32.83627760252366, "ram_util_percent": 42.12807570977918}}
{"episode_reward_max": 105.0, "episode_reward_min": 11.0, "episode_reward_mean": 30.32089552238806, "episode_len_mean": 30.32089552238806, "episode_media": {}, "episodes_this_iter": 134, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [43.0, 21.0, 11.0, 20.0, 17.0, 42.0, 15.0, 26.0, 18.0, 16.0, 95.0, 18.0, 15.0, 23.0, 70.0, 21.0, 14.0, 14.0, 53.0, 26.0, 30.0, 36.0, 105.0, 17.0, 28.0, 13.0, 29.0, 31.0, 97.0, 52.0, 30.0, 55.0, 15.0, 75.0, 45.0, 20.0, 25.0, 14.0, 23.0, 21.0, 23.0, 22.0, 17.0, 51.0, 36.0, 21.0, 26.0, 49.0, 15.0, 21.0, 66.0, 23.0, 41.0, 11.0, 26.0, 19.0, 15.0, 32.0, 26.0, 19.0, 32.0, 27.0, 23.0, 16.0, 38.0, 11.0, 45.0, 12.0, 41.0, 27.0, 86.0, 93.0, 17.0, 15.0, 33.0, 69.0, 19.0, 17.0, 21.0, 14.0, 26.0, 31.0, 13.0, 31.0, 49.0, 18.0, 19.0, 16.0, 16.0, 21.0, 37.0, 24.0, 20.0, 20.0, 32.0, 42.0, 20.0, 18.0, 55.0, 44.0, 23.0, 13.0, 27.0, 54.0, 13.0, 30.0, 22.0, 33.0, 28.0, 11.0, 41.0, 24.0, 16.0, 15.0, 39.0, 20.0, 33.0, 13.0, 66.0, 14.0, 44.0, 17.0, 21.0, 32.0, 12.0, 15.0, 14.0, 22.0, 66.0, 29.0, 46.0, 13.0, 26.0, 65.0], "episode_lengths": [43, 21, 11, 20, 17, 42, 15, 26, 18, 16, 95, 18, 15, 23, 70, 21, 14, 14, 53, 26, 30, 36, 105, 17, 28, 13, 29, 31, 97, 52, 30, 55, 15, 75, 45, 20, 25, 14, 23, 21, 23, 22, 17, 51, 36, 21, 26, 49, 15, 21, 66, 23, 41, 11, 26, 19, 15, 32, 26, 19, 32, 27, 23, 16, 38, 11, 45, 12, 41, 27, 86, 93, 17, 15, 33, 69, 19, 17, 21, 14, 26, 31, 13, 31, 49, 18, 19, 16, 16, 21, 37, 24, 20, 20, 32, 42, 20, 18, 55, 44, 23, 13, 27, 54, 13, 30, 22, 33, 28, 11, 41, 24, 16, 15, 39, 20, 33, 13, 66, 14, 44, 17, 21, 32, 12, 15, 14, 22, 66, 29, 46, 13, 26, 65]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.665987420269825, "mean_inference_ms": 2.967045297772866, "mean_action_processing_ms": 0.9127632370145304, "mean_env_wait_ms": 0.8048601741865865, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 36260, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [8e-10, 1.28e-08], "cur_lr": 0.001, "total_loss": 94.00726318359375, "policy_loss": -0.005941521376371384, "vf_loss": 188.0026397705078, "kl_loss": 0.016207648441195488, "inner_kl": [0.013508141972124577, 0.016668494790792465], "entropy": 0.588959813117981}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 1920000, "num_steps_sampled": 36260}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 103.65, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 49.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 179.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 96.7, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 46.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 178.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 115.225, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 32.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 171.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 111.175, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 38.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 183.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 117.25, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 46.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 175.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 111.9, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 61.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 176.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 107.625, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 60.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 187.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 91.65, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 45.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 163.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 95.075, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 51.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 172.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 94.975, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 46.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 153.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 84.125, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 41.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 164.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 84.15, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 39.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 167.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 106.85, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 57.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 178.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 101.975, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 44.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 190.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 113.625, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 74.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 177.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 116.8, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 40.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 128.325, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 73.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 121.725, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 62.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 195.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 113.8, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 63.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 192.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 129.725, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 57.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 192.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 114.725, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 67.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 167.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 112.9, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 58.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 165.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 120.025, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 60.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 168.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 116.475, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 53.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 175.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 90.725, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 37.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 180.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 102.95, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 50.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 192.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 129.25, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 87.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 174.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 121.525, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 79.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 162.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 131.975, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 91.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 174.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 125.975, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 77.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 190.0, "num_recreated_workers": 0, "done": false, "episodes_total": 1154, "training_iteration": 8, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-42-49", "timestamp": 1660817569, "time_this_iter_s": 268.6763644218445, "time_total_s": 2481.9871537685394, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2343a60>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d23433d0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 2481.9871537685394, "timesteps_since_restore": 128000, "iterations_since_restore": 8, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 29.308661417322835, "ram_util_percent": 41.01863517060367}}
{"episode_reward_max": 177.0, "episode_reward_min": 9.0, "episode_reward_mean": 32.096, "episode_len_mean": 32.096, "episode_media": {}, "episodes_this_iter": 125, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [23.0, 33.0, 19.0, 37.0, 18.0, 11.0, 50.0, 12.0, 30.0, 53.0, 57.0, 10.0, 39.0, 32.0, 25.0, 47.0, 11.0, 56.0, 18.0, 20.0, 12.0, 19.0, 19.0, 45.0, 43.0, 14.0, 42.0, 29.0, 26.0, 31.0, 34.0, 53.0, 23.0, 29.0, 29.0, 11.0, 17.0, 29.0, 20.0, 26.0, 12.0, 65.0, 24.0, 21.0, 58.0, 27.0, 47.0, 25.0, 33.0, 36.0, 28.0, 29.0, 27.0, 15.0, 17.0, 15.0, 17.0, 18.0, 22.0, 21.0, 19.0, 32.0, 18.0, 60.0, 23.0, 19.0, 24.0, 16.0, 124.0, 22.0, 32.0, 20.0, 30.0, 27.0, 20.0, 24.0, 42.0, 51.0, 25.0, 47.0, 14.0, 20.0, 12.0, 37.0, 10.0, 18.0, 43.0, 51.0, 31.0, 78.0, 12.0, 20.0, 41.0, 177.0, 39.0, 51.0, 33.0, 27.0, 25.0, 34.0, 90.0, 27.0, 15.0, 38.0, 19.0, 9.0, 36.0, 51.0, 16.0, 14.0, 16.0, 16.0, 36.0, 163.0, 14.0, 12.0, 24.0, 20.0, 42.0, 15.0, 82.0, 16.0, 38.0, 17.0, 29.0], "episode_lengths": [23, 33, 19, 37, 18, 11, 50, 12, 30, 53, 57, 10, 39, 32, 25, 47, 11, 56, 18, 20, 12, 19, 19, 45, 43, 14, 42, 29, 26, 31, 34, 53, 23, 29, 29, 11, 17, 29, 20, 26, 12, 65, 24, 21, 58, 27, 47, 25, 33, 36, 28, 29, 27, 15, 17, 15, 17, 18, 22, 21, 19, 32, 18, 60, 23, 19, 24, 16, 124, 22, 32, 20, 30, 27, 20, 24, 42, 51, 25, 47, 14, 20, 12, 37, 10, 18, 43, 51, 31, 78, 12, 20, 41, 177, 39, 51, 33, 27, 25, 34, 90, 27, 15, 38, 19, 9, 36, 51, 16, 14, 16, 16, 36, 163, 14, 12, 24, 20, 42, 15, 82, 16, 38, 17, 29]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.618512824698017, "mean_inference_ms": 2.9476646237215416, "mean_action_processing_ms": 0.900875913744647, "mean_env_wait_ms": 0.8014643309686262, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 40272, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [4.096e-07, 6.5536e-06], "cur_lr": 0.001, "total_loss": 39.85469436645508, "policy_loss": 0.0045541878789663315, "vf_loss": 79.71849822998047, "kl_loss": 0.007325717713683844, "inner_kl": [0.002736790105700493, 0.002918603830039501], "entropy": 0.5535479784011841}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 2160000, "num_steps_sampled": 40272}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 116.425, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 73.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 184.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 114.65, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 67.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 176.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 118.15, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 69.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 174.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 113.525, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 51.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 117.175, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 74.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 155.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 123.275, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 82.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 180.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 114.975, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 69.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 173.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 121.775, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 74.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 189.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 102.625, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 56.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 192.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 102.45, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 63.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 128.525, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 71.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 183.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 117.925, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 54.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 186.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 126.9, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 84.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 185.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 119.5, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 61.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 181.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 83.725, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 47.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 146.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 90.85, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 61.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 163.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 115.325, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 64.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 179.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 118.8, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 70.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 178.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 127.775, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 87.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 187.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 125.875, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 82.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 184.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 127.025, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 66.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 177.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 139.1, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 84.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 105.625, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 67.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 162.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 142.8, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 88.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 139.05, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 101.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 171.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 118.1, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 92.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 185.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 118.575, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 72.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 164.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 109.55, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 54.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 159.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 105.025, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 65.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 169.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 113.4, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 68.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 173.0, "num_recreated_workers": 0, "done": false, "episodes_total": 1279, "training_iteration": 9, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-47-44", "timestamp": 1660817864, "time_this_iter_s": 294.7952744960785, "time_total_s": 2776.782428264618, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d234b8b0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d234bc70>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 2776.782428264618, "timesteps_since_restore": 144000, "iterations_since_restore": 9, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 31.472792362768494, "ram_util_percent": 35.68806682577566}}
{"episode_reward_max": 116.0, "episode_reward_min": 10.0, "episode_reward_mean": 29.115942028985508, "episode_len_mean": 29.115942028985508, "episode_media": {}, "episodes_this_iter": 138, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [28.0, 33.0, 23.0, 18.0, 18.0, 15.0, 24.0, 47.0, 30.0, 45.0, 18.0, 24.0, 109.0, 59.0, 19.0, 67.0, 28.0, 36.0, 46.0, 23.0, 13.0, 23.0, 16.0, 32.0, 14.0, 40.0, 37.0, 19.0, 19.0, 18.0, 19.0, 16.0, 23.0, 21.0, 28.0, 23.0, 19.0, 13.0, 17.0, 58.0, 30.0, 33.0, 17.0, 36.0, 28.0, 17.0, 22.0, 19.0, 16.0, 46.0, 39.0, 13.0, 40.0, 34.0, 46.0, 12.0, 45.0, 54.0, 23.0, 15.0, 26.0, 16.0, 23.0, 29.0, 37.0, 24.0, 20.0, 22.0, 27.0, 21.0, 21.0, 16.0, 36.0, 22.0, 13.0, 13.0, 80.0, 30.0, 38.0, 52.0, 26.0, 30.0, 28.0, 23.0, 15.0, 37.0, 35.0, 23.0, 19.0, 20.0, 23.0, 19.0, 19.0, 15.0, 17.0, 26.0, 19.0, 55.0, 19.0, 17.0, 67.0, 25.0, 26.0, 23.0, 46.0, 66.0, 10.0, 18.0, 16.0, 50.0, 23.0, 116.0, 39.0, 20.0, 15.0, 21.0, 41.0, 13.0, 20.0, 23.0, 18.0, 62.0, 23.0, 34.0, 37.0, 16.0, 25.0, 20.0, 32.0, 23.0, 28.0, 27.0, 24.0, 18.0, 35.0, 28.0, 25.0, 51.0], "episode_lengths": [28, 33, 23, 18, 18, 15, 24, 47, 30, 45, 18, 24, 109, 59, 19, 67, 28, 36, 46, 23, 13, 23, 16, 32, 14, 40, 37, 19, 19, 18, 19, 16, 23, 21, 28, 23, 19, 13, 17, 58, 30, 33, 17, 36, 28, 17, 22, 19, 16, 46, 39, 13, 40, 34, 46, 12, 45, 54, 23, 15, 26, 16, 23, 29, 37, 24, 20, 22, 27, 21, 21, 16, 36, 22, 13, 13, 80, 30, 38, 52, 26, 30, 28, 23, 15, 37, 35, 23, 19, 20, 23, 19, 19, 15, 17, 26, 19, 55, 19, 17, 67, 25, 26, 23, 46, 66, 10, 18, 16, 50, 23, 116, 39, 20, 15, 21, 41, 13, 20, 23, 18, 62, 23, 34, 37, 16, 25, 20, 32, 23, 28, 27, 24, 18, 35, 28, 25, 51]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.478700521040937, "mean_inference_ms": 2.902952700245137, "mean_action_processing_ms": 0.8865797276399574, "mean_env_wait_ms": 0.7935714721679688, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 44290, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [1.31072e-05, 0.0001048576], "cur_lr": 0.001, "total_loss": 65.271240234375, "policy_loss": 0.0027400164399296045, "vf_loss": 130.5479736328125, "kl_loss": 0.01094220019876957, "inner_kl": [0.008800879120826721, 0.008800258859992027], "entropy": 0.5423538684844971}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 2400000, "num_steps_sampled": 44290}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 112.225, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 61.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 185.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 115.9, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 82.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 187.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 122.025, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 71.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 167.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 122.7, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 74.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 164.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 132.35, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 80.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 198.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 119.3, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 54.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 108.775, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 55.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 188.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 108.35, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 61.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 178.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 126.5, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 80.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 193.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 107.075, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 54.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 150.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 120.975, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 73.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 174.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 123.15, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 56.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 175.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 107.725, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 56.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 166.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 123.95, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 79.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 181.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 133.625, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 78.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 170.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 121.2, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 64.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 169.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 109.15, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 75.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 169.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 116.475, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 71.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 179.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 120.325, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 78.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 109.025, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 77.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 160.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 126.975, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 84.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 180.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 130.975, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 96.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 137.2, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 96.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 185.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 121.025, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 62.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 185.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 125.225, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 70.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 195.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 116.525, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 61.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 154.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 111.225, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 75.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 159.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 121.025, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 64.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 173.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 122.575, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 62.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 158.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 118.525, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 61.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 176.0, "num_recreated_workers": 0, "done": false, "episodes_total": 1417, "training_iteration": 10, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-54-02", "timestamp": 1660818242, "time_this_iter_s": 377.72539591789246, "time_total_s": 3154.5078241825104, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d23438e0>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d234ba90>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 3154.5078241825104, "timesteps_since_restore": 160000, "iterations_since_restore": 10, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 29.686219739292365, "ram_util_percent": 38.01564245810056}}
{"episode_reward_max": 183.0, "episode_reward_min": 10.0, "episode_reward_mean": 33.65546218487395, "episode_len_mean": 33.65546218487395, "episode_media": {}, "episodes_this_iter": 119, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [20.0, 183.0, 26.0, 35.0, 27.0, 60.0, 37.0, 45.0, 16.0, 16.0, 26.0, 63.0, 33.0, 92.0, 20.0, 18.0, 29.0, 26.0, 41.0, 105.0, 19.0, 26.0, 82.0, 13.0, 25.0, 24.0, 51.0, 30.0, 12.0, 22.0, 41.0, 17.0, 22.0, 28.0, 17.0, 15.0, 59.0, 41.0, 52.0, 34.0, 44.0, 20.0, 13.0, 13.0, 18.0, 39.0, 28.0, 40.0, 41.0, 21.0, 16.0, 13.0, 18.0, 13.0, 24.0, 58.0, 37.0, 13.0, 22.0, 10.0, 10.0, 18.0, 32.0, 22.0, 38.0, 14.0, 27.0, 21.0, 34.0, 15.0, 15.0, 41.0, 34.0, 58.0, 20.0, 25.0, 28.0, 39.0, 38.0, 21.0, 25.0, 103.0, 48.0, 14.0, 21.0, 18.0, 24.0, 25.0, 39.0, 48.0, 17.0, 34.0, 24.0, 110.0, 30.0, 36.0, 21.0, 47.0, 24.0, 17.0, 21.0, 29.0, 14.0, 45.0, 11.0, 29.0, 23.0, 25.0, 45.0, 57.0, 79.0, 57.0, 18.0, 14.0, 30.0, 11.0, 19.0, 59.0, 95.0], "episode_lengths": [20, 183, 26, 35, 27, 60, 37, 45, 16, 16, 26, 63, 33, 92, 20, 18, 29, 26, 41, 105, 19, 26, 82, 13, 25, 24, 51, 30, 12, 22, 41, 17, 22, 28, 17, 15, 59, 41, 52, 34, 44, 20, 13, 13, 18, 39, 28, 40, 41, 21, 16, 13, 18, 13, 24, 58, 37, 13, 22, 10, 10, 18, 32, 22, 38, 14, 27, 21, 34, 15, 15, 41, 34, 58, 20, 25, 28, 39, 38, 21, 25, 103, 48, 14, 21, 18, 24, 25, 39, 48, 17, 34, 24, 110, 30, 36, 21, 47, 24, 17, 21, 29, 14, 45, 11, 29, 23, 25, 45, 57, 79, 57, 18, 14, 30, 11, 19, 59, 95]}, "sampler_perf": {"mean_raw_obs_processing_ms": 10.400120670432887, "mean_inference_ms": 2.8635729521027, "mean_action_processing_ms": 0.8732995889292644, "mean_env_wait_ms": 0.7855989827230923, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "num_healthy_workers": 2, "timesteps_total": 48295, "timesteps_this_iter": 16000, "agent_timesteps_total": 0, "timers": {}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": [0.0008388608, 0.0067108864], "cur_lr": 0.001, "total_loss": 91.3636703491211, "policy_loss": -0.0029695816338062286, "vf_loss": 182.72137451171875, "kl_loss": 0.0041472772136330605, "inner_kl": [0.0030068648047745228, 0.0027811340987682343], "entropy": 0.5847019553184509}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 16000}}, "num_steps_trained_this_iter": 16000, "num_steps_trained": 2640000, "num_steps_sampled": 48295}, "MAMLIter0_DynaTrajInner_0_episode_reward_mean": 138.625, "MAMLIter0_DynaTrajInner_0_episode_reward_min": 100.0, "MAMLIter0_DynaTrajInner_0_episode_reward_max": 188.0, "MAMLIter0_DynaTrajInner_1_episode_reward_mean": 136.3, "MAMLIter0_DynaTrajInner_1_episode_reward_min": 96.0, "MAMLIter0_DynaTrajInner_1_episode_reward_max": 184.0, "MAMLIter1_DynaTrajInner_0_episode_reward_mean": 122.0, "MAMLIter1_DynaTrajInner_0_episode_reward_min": 52.0, "MAMLIter1_DynaTrajInner_0_episode_reward_max": 171.0, "MAMLIter1_DynaTrajInner_1_episode_reward_mean": 145.2, "MAMLIter1_DynaTrajInner_1_episode_reward_min": 106.0, "MAMLIter1_DynaTrajInner_1_episode_reward_max": 194.0, "MAMLIter2_DynaTrajInner_0_episode_reward_mean": 136.55, "MAMLIter2_DynaTrajInner_0_episode_reward_min": 102.0, "MAMLIter2_DynaTrajInner_0_episode_reward_max": 190.0, "MAMLIter2_DynaTrajInner_1_episode_reward_mean": 134.225, "MAMLIter2_DynaTrajInner_1_episode_reward_min": 99.0, "MAMLIter2_DynaTrajInner_1_episode_reward_max": 190.0, "MAMLIter3_DynaTrajInner_0_episode_reward_mean": 130.35, "MAMLIter3_DynaTrajInner_0_episode_reward_min": 73.0, "MAMLIter3_DynaTrajInner_0_episode_reward_max": 182.0, "MAMLIter3_DynaTrajInner_1_episode_reward_mean": 127.4, "MAMLIter3_DynaTrajInner_1_episode_reward_min": 76.0, "MAMLIter3_DynaTrajInner_1_episode_reward_max": 186.0, "MAMLIter4_DynaTrajInner_0_episode_reward_mean": 128.0, "MAMLIter4_DynaTrajInner_0_episode_reward_min": 60.0, "MAMLIter4_DynaTrajInner_0_episode_reward_max": 183.0, "MAMLIter4_DynaTrajInner_1_episode_reward_mean": 147.85, "MAMLIter4_DynaTrajInner_1_episode_reward_min": 86.0, "MAMLIter4_DynaTrajInner_1_episode_reward_max": 198.0, "MAMLIter5_DynaTrajInner_0_episode_reward_mean": 124.225, "MAMLIter5_DynaTrajInner_0_episode_reward_min": 57.0, "MAMLIter5_DynaTrajInner_0_episode_reward_max": 178.0, "MAMLIter5_DynaTrajInner_1_episode_reward_mean": 140.6, "MAMLIter5_DynaTrajInner_1_episode_reward_min": 81.0, "MAMLIter5_DynaTrajInner_1_episode_reward_max": 192.0, "MAMLIter6_DynaTrajInner_0_episode_reward_mean": 120.125, "MAMLIter6_DynaTrajInner_0_episode_reward_min": 70.0, "MAMLIter6_DynaTrajInner_0_episode_reward_max": 167.0, "MAMLIter6_DynaTrajInner_1_episode_reward_mean": 110.45, "MAMLIter6_DynaTrajInner_1_episode_reward_min": 60.0, "MAMLIter6_DynaTrajInner_1_episode_reward_max": 182.0, "MAMLIter7_DynaTrajInner_0_episode_reward_mean": 117.775, "MAMLIter7_DynaTrajInner_0_episode_reward_min": 56.0, "MAMLIter7_DynaTrajInner_0_episode_reward_max": 192.0, "MAMLIter7_DynaTrajInner_1_episode_reward_mean": 163.775, "MAMLIter7_DynaTrajInner_1_episode_reward_min": 100.0, "MAMLIter7_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter8_DynaTrajInner_0_episode_reward_mean": 103.15, "MAMLIter8_DynaTrajInner_0_episode_reward_min": 41.0, "MAMLIter8_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter8_DynaTrajInner_1_episode_reward_mean": 135.425, "MAMLIter8_DynaTrajInner_1_episode_reward_min": 91.0, "MAMLIter8_DynaTrajInner_1_episode_reward_max": 198.0, "MAMLIter9_DynaTrajInner_0_episode_reward_mean": 116.175, "MAMLIter9_DynaTrajInner_0_episode_reward_min": 74.0, "MAMLIter9_DynaTrajInner_0_episode_reward_max": 180.0, "MAMLIter9_DynaTrajInner_1_episode_reward_mean": 109.1, "MAMLIter9_DynaTrajInner_1_episode_reward_min": 65.0, "MAMLIter9_DynaTrajInner_1_episode_reward_max": 168.0, "MAMLIter10_DynaTrajInner_0_episode_reward_mean": 154.125, "MAMLIter10_DynaTrajInner_0_episode_reward_min": 97.0, "MAMLIter10_DynaTrajInner_0_episode_reward_max": 200.0, "MAMLIter10_DynaTrajInner_1_episode_reward_mean": 168.825, "MAMLIter10_DynaTrajInner_1_episode_reward_min": 95.0, "MAMLIter10_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter11_DynaTrajInner_0_episode_reward_mean": 125.775, "MAMLIter11_DynaTrajInner_0_episode_reward_min": 73.0, "MAMLIter11_DynaTrajInner_0_episode_reward_max": 194.0, "MAMLIter11_DynaTrajInner_1_episode_reward_mean": 168.6, "MAMLIter11_DynaTrajInner_1_episode_reward_min": 102.0, "MAMLIter11_DynaTrajInner_1_episode_reward_max": 200.0, "MAMLIter12_DynaTrajInner_0_episode_reward_mean": 127.15, "MAMLIter12_DynaTrajInner_0_episode_reward_min": 68.0, "MAMLIter12_DynaTrajInner_0_episode_reward_max": 179.0, "MAMLIter12_DynaTrajInner_1_episode_reward_mean": 134.925, "MAMLIter12_DynaTrajInner_1_episode_reward_min": 97.0, "MAMLIter12_DynaTrajInner_1_episode_reward_max": 193.0, "MAMLIter13_DynaTrajInner_0_episode_reward_mean": 136.0, "MAMLIter13_DynaTrajInner_0_episode_reward_min": 86.0, "MAMLIter13_DynaTrajInner_0_episode_reward_max": 188.0, "MAMLIter13_DynaTrajInner_1_episode_reward_mean": 134.125, "MAMLIter13_DynaTrajInner_1_episode_reward_min": 82.0, "MAMLIter13_DynaTrajInner_1_episode_reward_max": 189.0, "MAMLIter14_DynaTrajInner_0_episode_reward_mean": 143.45, "MAMLIter14_DynaTrajInner_0_episode_reward_min": 98.0, "MAMLIter14_DynaTrajInner_0_episode_reward_max": 194.0, "MAMLIter14_DynaTrajInner_1_episode_reward_mean": 122.225, "MAMLIter14_DynaTrajInner_1_episode_reward_min": 80.0, "MAMLIter14_DynaTrajInner_1_episode_reward_max": 189.0, "num_recreated_workers": 0, "done": false, "episodes_total": 1536, "training_iteration": 11, "trial_id": "9873a_00000", "experiment_id": "c97e19ec540d49bb8071b3e88ce59db6", "date": "2022-08-18_15-59-08", "timestamp": 1660818548, "time_this_iter_s": 305.9876871109009, "time_total_s": 3460.4955112934113, "pid": 16838, "hostname": "ajit-hp", "node_ip": "10.5.3.246", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "_fake_gpus": false, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": false, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "env": "myenv", "env_config": {}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "num_workers": 2, "num_envs_per_worker": 20, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "sample_async": false, "enable_connectors": false, "rollout_fragment_length": 200, "batch_mode": "complete_episodes", "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "validate_workers_after_construction": true, "ignore_worker_failures": false, "recreate_failed_workers": false, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "horizon": 200, "soft_horizon": false, "no_done_at_end": false, "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "synchronize_filters": true, "compress_observations": false, "enable_tf1_exec_eagerly": false, "sampler_perf_stats_ema_coef": null, "gamma": 0.99, "lr": 0.001, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [32, 32], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": true, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "explore": true, "exploration_config": {"type": "<class 'ray.rllib.algorithms.mbmpo.utils.MBMPOExploration'>", "random_timesteps": 8000}, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "evaluation_interval": null, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d2345370>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "off_policy_estimation_methods": {}, "evaluation_num_workers": 0, "always_attach_evaluation_results": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": -1, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "buffer_size": -1, "prioritized_replay": -1, "learning_starts": -1, "replay_batch_size": -1, "replay_sequence_length": null, "prioritized_replay_alpha": -1, "prioritized_replay_beta": -1, "prioritized_replay_eps": -1, "min_time_s_per_reporting": -1, "min_train_timesteps_per_reporting": -1, "min_sample_timesteps_per_reporting": -1, "input_evaluation": -1, "use_gae": true, "kl_coeff": 1e-10, "vf_loss_coeff": 0.5, "entropy_coeff": 0.0, "clip_param": 0.5, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.003, "inner_adaptation_steps": 1, "maml_optimizer_steps": 8, "inner_lr": 0.001, "dynamics_model": {"custom_model": "<class 'ray.rllib.algorithms.mbmpo.model_ensemble.DynamicsEnsembleCustomModel'>", "ensemble_size": 5, "fcnet_hiddens": [512, 512, 512], "lr": 0.001, "train_epochs": 500, "batch_size": 500, "valid_split_ratio": 0.2, "normalize_data": true}, "custom_vector_env": "<function model_vector_env at 0x7f07d2602c10>", "num_maml_steps": 15, "vf_share_layers": -1, "lambda": 1.0, "input": "sampler", "multiagent": {"policies": {"default_policy": "<ray.rllib.policy.policy.PolicySpec object at 0x7f07d242bf70>"}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": true, "custom_eval_function": null, "framework": "torch", "num_cpus_for_driver": 1}, "time_since_restore": 3460.4955112934113, "timesteps_since_restore": 176000, "iterations_since_restore": 11, "warmup_time": 396.12233686447144, "perf": {"cpu_util_percent": 24.44516129032258, "ram_util_percent": 37.75990783410139}}
